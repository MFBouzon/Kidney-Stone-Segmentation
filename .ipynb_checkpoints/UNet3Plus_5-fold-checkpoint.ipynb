{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0339d0-739a-4991-a2c0-4392d2ec6f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 00:39:34.053074: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-22 00:39:34.053121: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-22 00:39:34.053820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-22 00:39:34.057967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-22 00:39:34.816069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6f5337-7770-4f92-a2a4-251e31af3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "k = 5\n",
    "\n",
    "\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "data_ids = next(os.walk(DATA_PATH+'/image'))[2]\n",
    "\n",
    "X = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "# Definição do diretório de saída\n",
    "output_dir = \"UNet3Plus DICE 5-fold model\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Garante que a pasta existe\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_GPU_ALLOCATOR']= 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eeed634-f3f7-4cce-8c45-77e4b7c270ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 838/838 [00:19<00:00, 42.00it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Resizing training images and masks')\n",
    "for n, id_ in tqdm(enumerate(data_ids), total=len(data_ids)):   \n",
    "    path = DATA_PATH\n",
    "    img = imread(path + '/image/' + id_)[:,:]\n",
    "    img = img.reshape(img.shape[0], img.shape[1], IMG_CHANNELS)\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X[n] = img  #Fill empty X_train with values from img\n",
    "    \n",
    "    mask = imread(path + 'label/' + id_)\n",
    "    mask = (mask >= 250)\n",
    "    mask = np.expand_dims(resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n",
    "                                      preserve_range=True), axis=-1)\n",
    "    y[n] = mask \n",
    "    #plt.axis(\"off\")\n",
    "    #imshow(y[n])\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f02a06-5551-4109-97af-8d76f83beb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Calcula o F1-Score para segmentação binária\"\"\"\n",
    "    y_pred = K.round(y_pred)  # Arredondar para 0 ou 1\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float32'))  # Verdadeiros Positivos\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float32'))  # Falsos Positivos\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float32'))  # Falsos Negativos\n",
    "    \n",
    "    precision = tp / (tp + fp + K.epsilon())  # Precision\n",
    "    recall = tp / (tp + fn + K.epsilon())  # Recall\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())  # F1-Score\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e76775c-5fc9-419a-a36a-0bd07fe023ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return 1 - ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b9dd1e-626b-4787-b556-426246e4e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "k = 5\n",
    "\n",
    "\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "data_ids = next(os.walk(DATA_PATH+'/image'))[2]\n",
    "\n",
    "X = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_GPU_ALLOCATOR']= 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386e7353-1711-436b-93c8-9939c699dd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 838/838 [00:19<00:00, 42.46it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Resizing training images and masks')\n",
    "for n, id_ in tqdm(enumerate(data_ids), total=len(data_ids)):   \n",
    "    path = DATA_PATH\n",
    "    img = imread(path + '/image/' + id_)[:,:]\n",
    "    img = img.reshape(img.shape[0], img.shape[1], IMG_CHANNELS)\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X[n] = img  #Fill empty X_train with values from img\n",
    "    \n",
    "    mask = imread(path + 'label/' + id_)\n",
    "    mask = (mask >= 250)\n",
    "    mask = np.expand_dims(resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n",
    "                                      preserve_range=True), axis=-1)\n",
    "    y[n] = mask \n",
    "    #plt.axis(\"off\")\n",
    "    #imshow(y[n])\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f297ed-a66e-4727-b154-b60aa8f7fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, ReLU, MaxPooling2D, Conv2DTranspose, concatenate, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def unet_3_plus():\n",
    "    inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    \n",
    "    def conv_block(x, filters, dropout=0.1):\n",
    "        x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Conv2D(filters, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        return ReLU()(x)\n",
    "\n",
    "    # Encoder path\n",
    "    c1 = conv_block(inputs, 16)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = conv_block(p1, 32)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = conv_block(p2, 64, dropout=0.2)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = conv_block(p3, 128, dropout=0.2)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = conv_block(p4, 256, dropout=0.3)  # Bottleneck\n",
    "\n",
    "    # UNet3+ Decoder - Full-Scale Skip Connections\n",
    "    def full_scale_skip(*layers):\n",
    "        upsampled_layers = [UpSampling2D(size=(2 ** i, 2 ** i))(layer) for i, layer in enumerate(layers)]\n",
    "        return concatenate(upsampled_layers)\n",
    "\n",
    "    u6 = full_scale_skip(c1, c2, c3, c4, c5)\n",
    "    c6 = conv_block(u6, 128)\n",
    "\n",
    "    u7 = full_scale_skip(c1, c2, c3, c6, c5)\n",
    "    c7 = conv_block(u7, 64)\n",
    "\n",
    "    u8 = full_scale_skip(c1, c2, c7, c6, c5)\n",
    "    c8 = conv_block(u8, 32)\n",
    "\n",
    "    u9 = full_scale_skip(c1, c8, c7, c6, c5)\n",
    "    c9 = conv_block(u9, 16)\n",
    "\n",
    "    # Final output layer\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss=dice_loss, metrics=[\n",
    "        'accuracy', \n",
    "        tf.keras.metrics.Recall(name='recall'), \n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        f1_score,\n",
    "        tf.keras.metrics.IoU(num_classes=2, target_class_ids={0,1}, name='IoU')\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5bb3dc3-8cc2-41d0-bf67-a27f9775446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 256, 256, 16), (None, 512, 512, 16)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(fold))\n\u001b[1;32m     19\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43munet3_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mfold.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     27\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m             save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36munet3_plus\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m u1 \u001b[38;5;241m=\u001b[39m UpSampling2D(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))(c5)\n\u001b[1;32m     32\u001b[0m u1 \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m16\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(u1)\n\u001b[0;32m---> 33\u001b[0m u1 \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m u2 \u001b[38;5;241m=\u001b[39m UpSampling2D(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))(c5)\n\u001b[1;32m     36\u001b[0m u2 \u001b[38;5;241m=\u001b[39m Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(u2)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/merging/concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    125\u001b[0m unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    126\u001b[0m     shape[axis]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 256, 256, 16), (None, 512, 512, 16)]"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "jacc = []\n",
    "f1 = []\n",
    "prec = []\n",
    "rec = []\n",
    "\n",
    "# Dicionário para armazenar métricas por fold\n",
    "fold_metrics_summary = []\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "fold = 1\n",
    "\n",
    "for i, (train_ind, test_ind) in enumerate(kf.split(X)):\n",
    "    if i < (fold-1):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    print(\"Fold: \" + str(fold))\n",
    "    X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "\n",
    "    model = unet3_plus()\n",
    "    \n",
    "    \n",
    "    checkpoint_filepath = os.path.join(output_dir, f'model_{fold}fold.keras')\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=False,\n",
    "                monitor='val_f1_score',\n",
    "                mode='max',\n",
    "                save_best_only=True,\n",
    "                verbose=1)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), batch_size=16, epochs=150, callbacks=callbacks)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "     # Salvando o tempo de treinamento\n",
    "    with open(os.path.join(output_dir, 'training_time.txt'), 'a') as f:\n",
    "        f.write(f'Fold {fold}: {training_time:.2f} segundos\\n')\n",
    "    print(f\"O modelo demorou {training_time:.2f} segundos para treinar.\")\n",
    "    \n",
    "    loss = model.history.history['loss']\n",
    "    val_loss = model.history.history['val_loss']\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'loss_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Epoch\", \"Loss\", \"Validation Loss\"])\n",
    "        for epoch, (l, vl) in enumerate(zip(loss, val_loss), start=1):\n",
    "            writer.writerow([epoch, l, vl])\n",
    "            \n",
    "     # Plotando e salvando a figura\n",
    "    plt.figure()\n",
    "    plt.plot(loss, 'r', label='Training loss')\n",
    "    plt.plot(val_loss, 'g', label='Validation loss')\n",
    "    plt.title(f'Training and Validation Loss - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_plot_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    acc_fold = []\n",
    "    jacc_fold = []\n",
    "    f1_fold = []\n",
    "    prec_fold = []\n",
    "    rec_fold = []\n",
    "    \n",
    "    for i in range(0, len(X_val)):\n",
    "        sample_image = X_val[i]\n",
    "        sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "        prediction = model.predict(sample_image[tf.newaxis, ...],verbose=0)[0]\n",
    "        predicted_mask = (prediction > 0.5).astype(np.uint8).flatten()\n",
    "            \n",
    "        acc_score = accuracy_score(sample_mask, predicted_mask)\n",
    "        jacc_score = jaccard_score(sample_mask, predicted_mask)\n",
    "        f1_score_val = f1_score(sample_mask, predicted_mask).numpy()\n",
    "        prec_score = precision_score(sample_mask, predicted_mask)\n",
    "        rec_score = recall_score(sample_mask, predicted_mask)\n",
    "\n",
    "        acc_fold.append(acc_score)\n",
    "        jacc_fold.append(jacc_score)\n",
    "        f1_fold.append(f1_score_val)\n",
    "        prec_fold.append(prec_score)\n",
    "        rec_fold.append(rec_score)\n",
    "\n",
    "        fold_metrics.append([i, acc_score, jacc_score, f1_score_val, prec_score, rec_score])\n",
    "        \n",
    "    # Salvando métricas por imagem\n",
    "    with open(os.path.join(output_dir, f'metrics_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Index\", \"Accuracy\", \"Jaccard\", \"DICE\", \"Precision\", \"Recall\"])\n",
    "        writer.writerows(fold_metrics)\n",
    "\n",
    "    # Salvando a média do fold para cada métrica\n",
    "    fold_metrics_summary.append([\n",
    "        fold, np.mean(acc_fold), np.std(acc_fold),\n",
    "        np.mean(jacc_fold), np.std(jacc_fold),\n",
    "        np.mean(f1_fold), np.std(f1_fold),\n",
    "        np.mean(prec_fold), np.std(prec_fold),\n",
    "        np.mean(rec_fold), np.std(rec_fold)\n",
    "    ])\n",
    "\n",
    "        \n",
    "    del model \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    fold+=1\n",
    "    \n",
    "# Calculando médias gerais\n",
    "acc_total = [m[1] for m in fold_metrics_summary]\n",
    "jacc_total = [m[3] for m in fold_metrics_summary]\n",
    "f1_total = [m[5] for m in fold_metrics_summary]\n",
    "prec_total = [m[7] for m in fold_metrics_summary]\n",
    "rec_total = [m[9] for m in fold_metrics_summary]\n",
    "\n",
    "general_summary = [\n",
    "    [\"Fold\", \"Accuracy Mean\", \"Accuracy Std\", \"Jaccard Mean\", \"Jaccard Std\",\n",
    "     \"DICE Mean\", \"DICE Std\", \"Precision Mean\", \"Precision Std\", \"Recall Mean\", \"Recall Std\"]\n",
    "]\n",
    "\n",
    "general_summary.extend(fold_metrics_summary)\n",
    "\n",
    "general_summary.append([\n",
    "    \"Overall Mean\", np.mean(acc_total), np.std(acc_total),\n",
    "    np.mean(jacc_total), np.std(jacc_total),\n",
    "    np.mean(f1_total), np.std(f1_total),\n",
    "    np.mean(prec_total), np.std(prec_total),\n",
    "    np.mean(rec_total), np.std(rec_total)\n",
    "])\n",
    "\n",
    "# Salvando o resumo das métricas por fold e geral\n",
    "with open(os.path.join(output_dir, 'metrics_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    writer.writerows(general_summary)\n",
    "\n",
    "print(\"Métricas salvas com sucesso na pasta:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d184f-5b31-4156-849f-d24ddac7c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mean_fold = []\n",
    "jacc_mean_fold = []\n",
    "f1_mean_fold = []\n",
    "prec_mean_fold = []\n",
    "rec_mean_fold = []\n",
    "\n",
    "acc_std_fold = []\n",
    "jacc_std_fold = []\n",
    "f1_std_fold = []\n",
    "prec_std_fold = []\n",
    "rec_std_fold = []\n",
    "\n",
    "\n",
    "# Dicionário para armazenar métricas por fold\n",
    "fold_metrics_summary = []\n",
    "\n",
    "fold = 1\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "for train_ind, test_ind in tqdm(kf.split(X), total=kf.get_n_splits(), desc=\"k-fold\"):\n",
    "    X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    acc_fold = []\n",
    "    jacc_fold = []\n",
    "    f1_fold = []\n",
    "    prec_fold = []\n",
    "    rec_fold = []\n",
    "    \n",
    "    model_filepath = filepath = os.path.join(output_dir, f'model_{fold}fold.keras')\n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "\n",
    "    for i in range(0, len(X_val)):\n",
    "        sample_image = X_val[i]\n",
    "        sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "        prediction = model.predict(sample_image[tf.newaxis, ...],verbose=0)[0]\n",
    "        predicted_mask = (prediction > 0.5).astype(np.uint8).flatten()\n",
    "            \n",
    "        acc_score = accuracy_score(sample_mask, predicted_mask)\n",
    "        jacc_score = jaccard_score(sample_mask, predicted_mask)\n",
    "        f1_score_val = f1_score(sample_mask, predicted_mask).numpy()\n",
    "        prec_score = precision_score(sample_mask, predicted_mask)\n",
    "        rec_score = recall_score(sample_mask, predicted_mask)\n",
    "\n",
    "        acc_fold.append(acc_score)\n",
    "        jacc_fold.append(jacc_score)\n",
    "        f1_fold.append(f1_score_val)\n",
    "        prec_fold.append(prec_score)\n",
    "        rec_fold.append(rec_score)\n",
    "\n",
    "        fold_metrics.append([i, acc_score, jacc_score, f1_score_val, prec_score, rec_score])\n",
    "        \n",
    "    # Salvando métricas por imagem\n",
    "    with open(os.path.join(output_dir, f'metrics_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Index\", \"Accuracy\", \"Jaccard\", \"DICE\", \"Precision\", \"Recall\"])\n",
    "        writer.writerows(fold_metrics)\n",
    "\n",
    "    # Salvando a média do fold para cada métrica\n",
    "    fold_metrics_summary.append([\n",
    "        fold, np.mean(acc_fold), np.std(acc_fold),\n",
    "        np.mean(jacc_fold), np.std(jacc_fold),\n",
    "        np.mean(f1_fold), np.std(f1_fold),\n",
    "        np.mean(prec_fold), np.std(prec_fold),\n",
    "        np.mean(rec_fold), np.std(rec_fold)\n",
    "    ])\n",
    "\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Calculando médias gerais\n",
    "acc_total = [m[1] for m in fold_metrics_summary]\n",
    "jacc_total = [m[3] for m in fold_metrics_summary]\n",
    "f1_total = [m[5] for m in fold_metrics_summary]\n",
    "prec_total = [m[7] for m in fold_metrics_summary]\n",
    "rec_total = [m[9] for m in fold_metrics_summary]\n",
    "\n",
    "general_summary = [\n",
    "    [\"Fold\", \"Accuracy Mean\", \"Accuracy Std\", \"Jaccard Mean\", \"Jaccard Std\",\n",
    "     \"DICE Mean\", \"DICE Std\", \"Precision Mean\", \"Precision Std\", \"Recall Mean\", \"Recall Std\"]\n",
    "]\n",
    "\n",
    "general_summary.extend(fold_metrics_summary)\n",
    "\n",
    "general_summary.append([\n",
    "    \"Overall Mean\", np.mean(acc_total), np.std(acc_total),\n",
    "    np.mean(jacc_total), np.std(jacc_total),\n",
    "    np.mean(f1_total), np.std(f1_total),\n",
    "    np.mean(prec_total), np.std(prec_total),\n",
    "    np.mean(rec_total), np.std(rec_total)\n",
    "])\n",
    "\n",
    "# Salvando o resumo das métricas por fold e geral\n",
    "with open(os.path.join(output_dir, 'metrics_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    writer.writerows(general_summary)\n",
    "\n",
    "print(\"Métricas salvas com sucesso na pasta:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2544ddf-696d-4ff5-8780-9a498daa8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \"+ str(np.mean(acc_total)*100) + \" +- \" + str(np.std(acc_total)*100))\n",
    "print(\"Jaccard: \"+ str(np.mean(jacc_total)*100) + \" +- \" + str(np.std(jacc_total)*100))\n",
    "print(\"Dice: \"+ str(np.mean(f1_total)*100) + \" +- \" + str(np.std(f1_total)*100))\n",
    "print(\"Precision: \"+ str(np.mean(prec_total)*100) + \" +- \" + str(np.std(prec_total)*100))\n",
    "print(\"Recall: \"+ str(np.mean(rec_total)*100) + \" +- \" + str(np.std(rec_total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774436e-ff52-47f8-8dce-d80b378b614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = 2\n",
    "best_model_filepath = filepath = os.path.join(output_dir, f'model_{best_fold}fold.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac69b19-a5f7-40f1-b395-d65cd72999d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "for i, (train_ind, test_ind) in enumerate(kf.split(X)):\n",
    "    if i == (best_fold-1):\n",
    "        X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6713b66-5fa4-436e-a338-dee9851b3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input image', 'True mask', 'Predicted mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d56e2-63cb-4232-b7eb-0a9ed7c6b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(X_val))\n",
    "sample_image = X_val[i]\n",
    "sample_mask = y_val[i]\n",
    "prediction = best_model.predict(sample_image[tf.newaxis, ...])[0]\n",
    "predicted_mask = (prediction > 0.5).astype(np.uint8)\n",
    "display([sample_image, sample_mask,predicted_mask])\n",
    "\n",
    "sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "predicted_mask = predicted_mask.flatten()\n",
    "\n",
    "\n",
    "print(\"Accuracy: \"+ str(accuracy_score(sample_mask, predicted_mask)))\n",
    "print(\"Jaccard: \"+ str(jaccard_score(sample_mask, predicted_mask)))\n",
    "print(\"Dice: \"+ str((2*jaccard_score(sample_mask, predicted_mask))/(1+jaccard_score(sample_mask, predicted_mask))))\n",
    "print(\"Precision: \"+ str(precision_score(sample_mask, predicted_mask)))\n",
    "print(\"Recall: \"+ str(recall_score(sample_mask, predicted_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd9e15-bc6d-4743-ae4b-ce8a3202b64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
