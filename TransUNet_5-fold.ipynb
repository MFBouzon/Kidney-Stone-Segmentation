{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0339d0-739a-4991-a2c0-4392d2ec6f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 14:51:03.727069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-26 14:51:03.727113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-26 14:51:03.727952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-26 14:51:03.732099: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 14:51:04.232618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    " \n",
    "from tqdm import tqdm \n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6f5337-7770-4f92-a2a4-251e31af3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "k = 5\n",
    "\n",
    "\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "\n",
    "data_ids = next(os.walk(DATA_PATH+'/image'))[2]\n",
    "\n",
    "X = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "y = np.zeros((len(data_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)\n",
    "\n",
    "# Definição do diretório de saída\n",
    "output_dir = \"Trans UNet DICE 5-fold model\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Garante que a pasta existe\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_GPU_ALLOCATOR']= 'cuda_malloc_async'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eeed634-f3f7-4cce-8c45-77e4b7c270ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 838/838 [00:20<00:00, 41.50it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Resizing training images and masks')\n",
    "for n, id_ in tqdm(enumerate(data_ids), total=len(data_ids)):   \n",
    "    path = DATA_PATH\n",
    "    img = imread(path + '/image/' + id_)[:,:]\n",
    "    img = img.reshape(img.shape[0], img.shape[1], IMG_CHANNELS)\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X[n] = img  #Fill empty X_train with values from img\n",
    "    \n",
    "    mask = imread(path + 'label/' + id_)\n",
    "    mask = (mask >= 250)\n",
    "    mask = np.expand_dims(resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant',  \n",
    "                                      preserve_range=True), axis=-1)\n",
    "    y[n] = mask \n",
    "    #plt.axis(\"off\")\n",
    "    #imshow(y[n])\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f02a06-5551-4109-97af-8d76f83beb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"Calcula o F1-Score para segmentação binária\"\"\"\n",
    "    y_pred = K.round(y_pred)  # Arredondar para 0 ou 1\n",
    "    tp = K.sum(K.cast(y_true * y_pred, 'float32'))  # Verdadeiros Positivos\n",
    "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float32'))  # Falsos Positivos\n",
    "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float32'))  # Falsos Negativos\n",
    "    \n",
    "    precision = tp / (tp + fp + K.epsilon())  # Precision\n",
    "    recall = tp / (tp + fn + K.epsilon())  # Recall\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())  # F1-Score\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e76775c-5fc9-419a-a36a-0bd07fe023ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return 1 - ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f297ed-a66e-4727-b154-b60aa8f7fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, Dropout, BatchNormalization, ReLU, MaxPooling2D,\n",
    "    Conv2DTranspose, Concatenate, Reshape, Dense, LayerNormalization,\n",
    "    MultiHeadAttention, Input\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patches, d_model):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pos_emb = self.add_weight(\n",
    "            shape=(1, num_patches, d_model),\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='positional_embedding'\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_emb\n",
    "\n",
    "def trans_unet():\n",
    "    input_shape =  (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    def conv_block(x, filters, dropout=0.1):\n",
    "        x = Conv2D(filters, (3, 3), activation='relu', \n",
    "                 kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "        x = Conv2D(filters, (3, 3), activation='relu', \n",
    "                 kernel_initializer='he_normal', padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        return ReLU()(x)\n",
    "    \n",
    "    # Encoder (CNN)\n",
    "    c1 = conv_block(inputs, 16)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = conv_block(p1, 32)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    \n",
    "    c3 = conv_block(p2, 64, dropout=0.2)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    \n",
    "    c4 = conv_block(p3, 128, dropout=0.2)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    \n",
    "    # Transformer Bottleneck\n",
    "    h = input_shape[0] // 16\n",
    "    w = input_shape[1] // 16\n",
    "    num_patches = h * w\n",
    "    d_model = 256\n",
    "    \n",
    "    x = Reshape((num_patches, 128))(p4)\n",
    "    x = Dense(d_model)(x)\n",
    "    x = PositionalEmbedding(num_patches, d_model)(x)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    num_heads = 8\n",
    "    for _ in range(4):\n",
    "        x1 = LayerNormalization()(x)\n",
    "        attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(x1, x1)\n",
    "        x = attn + x\n",
    "        x2 = LayerNormalization()(x)\n",
    "        mlp = Dense(d_model * 4, activation='gelu')(x2)\n",
    "        mlp = Dense(d_model)(mlp)\n",
    "        x = mlp + x\n",
    "    \n",
    "    x = Reshape((h, w, d_model))(x)\n",
    "    \n",
    "    # Decoder (UNet++ style)\n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(x)\n",
    "    u6 = Concatenate()([u6, c4])\n",
    "    c6 = conv_block(u6, 128)\n",
    "    \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = Concatenate()([u7, c3])\n",
    "    c7 = conv_block(u7, 64)\n",
    "    \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = Concatenate()([u8, c2])\n",
    "    c8 = conv_block(u8, 32)\n",
    "    \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = Concatenate()([u9, c1])\n",
    "    c9 = conv_block(u9, 16)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss=dice_loss, metrics=[\n",
    "        'accuracy', \n",
    "        tf.keras.metrics.Recall(name='recall'), \n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        f1_score,\n",
    "        tf.keras.metrics.IoU(num_classes=2, target_class_ids={0,1}, name='IoU')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bb3dc3-8cc2-41d0-bf67-a27f9775446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:28:57.613866: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.643213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.643250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.644593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.644624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.644638: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.759689: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.759737: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.759744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-03-24 23:28:57.759754: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-24 23:28:57.759759: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-03-24 23:28:57.759865: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-24 23:28:57.759882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 23:29:03.550644: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-03-24 23:29:04.275230: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2025-03-24 23:29:08.278949: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:306] gpu_async_0 cuMemAllocAsync failed to allocate 1828790272 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\n",
      " Reported by CUDA: Free memory/Total memory: 501219328/12878086144\n",
      "2025-03-24 23:29:08.278990: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:311] Stats: Limit:                     10022289408\n",
      "InUse:                     10531789734\n",
      "MaxInUse:                  11353891766\n",
      "NumAllocs:                        1627\n",
      "MaxAllocSize:               2722250752\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-03-24 23:29:08.279017: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:63] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-03-24 23:29:08.279020: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1, 22\n",
      "2025-03-24 23:29:08.279022: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4, 57\n",
      "2025-03-24 23:29:08.279023: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8, 9\n",
      "2025-03-24 23:29:08.279025: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16, 2\n",
      "2025-03-24 23:29:08.279026: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 64, 36\n",
      "2025-03-24 23:29:08.279027: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 128, 35\n",
      "2025-03-24 23:29:08.279029: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 256, 35\n",
      "2025-03-24 23:29:08.279030: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 512, 35\n",
      "2025-03-24 23:29:08.279031: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 576, 3\n",
      "2025-03-24 23:29:08.279033: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1024, 111\n",
      "2025-03-24 23:29:08.279034: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1028, 1\n",
      "2025-03-24 23:29:08.279035: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4096, 12\n",
      "2025-03-24 23:29:08.279037: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8192, 3\n",
      "2025-03-24 23:29:08.279038: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 9216, 6\n",
      "2025-03-24 23:29:08.279040: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 18432, 7\n",
      "2025-03-24 23:29:08.279041: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 32768, 3\n",
      "2025-03-24 23:29:08.279042: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 36864, 6\n",
      "2025-03-24 23:29:08.279044: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 65536, 24\n",
      "2025-03-24 23:29:08.279045: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 73728, 6\n",
      "2025-03-24 23:29:08.279046: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 131072, 6\n",
      "2025-03-24 23:29:08.279048: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 147456, 6\n",
      "2025-03-24 23:29:08.279049: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 262144, 48\n",
      "2025-03-24 23:29:08.279050: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 294912, 6\n",
      "2025-03-24 23:29:08.279052: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 524288, 3\n",
      "2025-03-24 23:29:08.279053: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 589824, 6\n",
      "2025-03-24 23:29:08.279054: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1048576, 27\n",
      "2025-03-24 23:29:08.279056: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 1179648, 3\n",
      "2025-03-24 23:29:08.279057: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 4194304, 2\n",
      "2025-03-24 23:29:08.279058: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 8388608, 3\n",
      "2025-03-24 23:29:08.279060: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 16777216, 47\n",
      "2025-03-24 23:29:08.279061: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 33554432, 11\n",
      "2025-03-24 23:29:08.279062: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 67108864, 27\n",
      "2025-03-24 23:29:08.279064: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 134217728, 9\n",
      "2025-03-24 23:29:08.279065: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 175898624, 1\n",
      "2025-03-24 23:29:08.279066: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 268435456, 7\n",
      "2025-03-24 23:29:08.279068: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 285212672, 1\n",
      "2025-03-24 23:29:08.279069: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 536870912, 6\n",
      "2025-03-24 23:29:08.279070: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:66] 703594496, 1\n",
      "2025-03-24 23:29:08.279074: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:97] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 11106516992\n",
      "2025-03-24 23:29:08.279076: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:99] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 10531789734\n",
      "2025-03-24 23:29:08.279077: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:100] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 12952010752\n",
      "2025-03-24 23:29:08.279079: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:101] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 11353891766\n",
      "2025-03-24 23:29:09.119946: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3a7c64b9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-03-24 23:29:09.119980: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070, Compute Capability 8.9\n",
      "2025-03-24 23:29:09.124616: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742869749.190536  168767 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - ETA: 0s - loss: 0.9972 - accuracy: 0.7281 - recall: 0.9564 - precision: 0.0022 - f1_score: 0.0046 - IoU: 0.5026\n",
      "Epoch 1: val_f1_score improved from -inf to 0.00185, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 99s 1s/step - loss: 0.9972 - accuracy: 0.7281 - recall: 0.9564 - precision: 0.0022 - f1_score: 0.0046 - IoU: 0.5026 - val_loss: 0.9982 - val_accuracy: 0.4013 - val_recall: 1.0000 - val_precision: 9.7605e-04 - val_f1_score: 0.0018 - val_IoU: 0.2893\n",
      "Epoch 2/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9945 - accuracy: 0.9466 - recall: 0.9913 - precision: 0.0117 - f1_score: 0.0420 - IoU: 0.5399\n",
      "Epoch 2: val_f1_score improved from 0.00185 to 0.00330, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 31s 736ms/step - loss: 0.9945 - accuracy: 0.9466 - recall: 0.9913 - precision: 0.0117 - f1_score: 0.0420 - IoU: 0.5399 - val_loss: 0.9967 - val_accuracy: 0.6680 - val_recall: 1.0000 - val_precision: 0.0018 - val_f1_score: 0.0033 - val_IoU: 0.3867\n",
      "Epoch 3/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9897 - accuracy: 0.9891 - recall: 0.9874 - precision: 0.0548 - f1_score: 0.1363 - IoU: 0.5953\n",
      "Epoch 3: val_f1_score improved from 0.00330 to 0.02761, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 572ms/step - loss: 0.9897 - accuracy: 0.9891 - recall: 0.9874 - precision: 0.0548 - f1_score: 0.1363 - IoU: 0.5953 - val_loss: 0.9790 - val_accuracy: 0.9611 - val_recall: 0.9757 - val_precision: 0.0145 - val_f1_score: 0.0276 - val_IoU: 0.5654\n",
      "Epoch 4/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9789 - accuracy: 0.9937 - recall: 0.9737 - precision: 0.0897 - f1_score: 0.2195 - IoU: 0.6288\n",
      "Epoch 4: val_f1_score did not improve from 0.02761\n",
      "42/42 [==============================] - 28s 676ms/step - loss: 0.9789 - accuracy: 0.9937 - recall: 0.9737 - precision: 0.0897 - f1_score: 0.2195 - IoU: 0.6288 - val_loss: 0.9988 - val_accuracy: 0.6384 - val_recall: 0.3369 - val_precision: 5.4528e-04 - val_f1_score: 0.0011 - val_IoU: 0.4976\n",
      "Epoch 5/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9404 - accuracy: 0.9976 - recall: 0.9535 - precision: 0.2046 - f1_score: 0.4019 - IoU: 0.6831\n",
      "Epoch 5: val_f1_score improved from 0.02761 to 0.02843, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 26s 633ms/step - loss: 0.9404 - accuracy: 0.9976 - recall: 0.9535 - precision: 0.2046 - f1_score: 0.4019 - IoU: 0.6831 - val_loss: 0.9885 - val_accuracy: 0.9983 - val_recall: 0.0555 - val_precision: 0.0273 - val_f1_score: 0.0284 - val_IoU: 0.5096\n",
      "Epoch 6/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7884 - accuracy: 0.9994 - recall: 0.9324 - precision: 0.5242 - f1_score: 0.6707 - IoU: 0.8035\n",
      "Epoch 6: val_f1_score improved from 0.02843 to 0.27428, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 564ms/step - loss: 0.7884 - accuracy: 0.9994 - recall: 0.9324 - precision: 0.5242 - f1_score: 0.6707 - IoU: 0.8035 - val_loss: 0.7585 - val_accuracy: 0.9993 - val_recall: 0.2978 - val_precision: 0.3850 - val_f1_score: 0.2743 - val_IoU: 0.5735\n",
      "Epoch 7/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.9997 - recall: 0.8717 - precision: 0.7188 - f1_score: 0.7854 - IoU: 0.8293\n",
      "Epoch 7: val_f1_score improved from 0.27428 to 0.48641, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 36s 870ms/step - loss: 0.4933 - accuracy: 0.9997 - recall: 0.8717 - precision: 0.7188 - f1_score: 0.7854 - IoU: 0.8293 - val_loss: 0.5563 - val_accuracy: 0.9984 - val_recall: 0.9135 - val_precision: 0.2582 - val_f1_score: 0.4864 - val_IoU: 0.6777\n",
      "Epoch 8/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2406 - accuracy: 0.9998 - recall: 0.8953 - precision: 0.8701 - f1_score: 0.8796 - IoU: 0.8660\n",
      "Epoch 8: val_f1_score did not improve from 0.48641\n",
      "42/42 [==============================] - 45s 1s/step - loss: 0.2406 - accuracy: 0.9998 - recall: 0.8953 - precision: 0.8701 - f1_score: 0.8796 - IoU: 0.8660 - val_loss: 0.7730 - val_accuracy: 0.9995 - val_recall: 0.1538 - val_precision: 0.9982 - val_f1_score: 0.2222 - val_IoU: 0.5025\n",
      "Epoch 9/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9999 - recall: 0.9075 - precision: 0.9026 - f1_score: 0.9037 - IoU: 0.8801\n",
      "Epoch 9: val_f1_score improved from 0.48641 to 0.59036, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 561ms/step - loss: 0.1566 - accuracy: 0.9999 - recall: 0.9075 - precision: 0.9026 - f1_score: 0.9037 - IoU: 0.8801 - val_loss: 0.4176 - val_accuracy: 0.9997 - val_recall: 0.5135 - val_precision: 0.9663 - val_f1_score: 0.5904 - val_IoU: 0.6372\n",
      "Epoch 10/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9999 - recall: 0.9065 - precision: 0.9002 - f1_score: 0.9009 - IoU: 0.8818\n",
      "Epoch 10: val_f1_score improved from 0.59036 to 0.83112, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 23s 545ms/step - loss: 0.1363 - accuracy: 0.9999 - recall: 0.9065 - precision: 0.9002 - f1_score: 0.9009 - IoU: 0.8818 - val_loss: 0.1930 - val_accuracy: 0.9998 - val_recall: 0.9131 - val_precision: 0.8336 - val_f1_score: 0.8311 - val_IoU: 0.8494\n",
      "Epoch 11/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9999 - recall: 0.9261 - precision: 0.9243 - f1_score: 0.9225 - IoU: 0.8952\n",
      "Epoch 11: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 492ms/step - loss: 0.1030 - accuracy: 0.9999 - recall: 0.9261 - precision: 0.9243 - f1_score: 0.9225 - IoU: 0.8952 - val_loss: 0.9922 - val_accuracy: 0.9994 - val_recall: 0.0049 - val_precision: 1.0000 - val_f1_score: 0.0077 - val_IoU: 0.4997\n",
      "Epoch 12/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9999 - recall: 0.9341 - precision: 0.9391 - f1_score: 0.9349 - IoU: 0.9044\n",
      "Epoch 12: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 20s 488ms/step - loss: 0.0853 - accuracy: 0.9999 - recall: 0.9341 - precision: 0.9391 - f1_score: 0.9349 - IoU: 0.9044 - val_loss: 0.3556 - val_accuracy: 0.9998 - val_recall: 0.6226 - val_precision: 0.9916 - val_f1_score: 0.7096 - val_IoU: 0.6562\n",
      "Epoch 13/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9999 - recall: 0.9406 - precision: 0.9430 - f1_score: 0.9393 - IoU: 0.9109\n",
      "Epoch 13: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 20s 488ms/step - loss: 0.0770 - accuracy: 0.9999 - recall: 0.9406 - precision: 0.9430 - f1_score: 0.9393 - IoU: 0.9109 - val_loss: 0.7677 - val_accuracy: 0.9995 - val_recall: 0.1389 - val_precision: 0.9878 - val_f1_score: 0.2868 - val_IoU: 0.5036\n",
      "Epoch 14/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9999 - recall: 0.9541 - precision: 0.9503 - f1_score: 0.9521 - IoU: 0.9217\n",
      "Epoch 14: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 20s 487ms/step - loss: 0.0612 - accuracy: 0.9999 - recall: 0.9541 - precision: 0.9503 - f1_score: 0.9521 - IoU: 0.9217 - val_loss: 0.9532 - val_accuracy: 0.9994 - val_recall: 0.0257 - val_precision: 1.0000 - val_f1_score: 0.0663 - val_IoU: 0.4998\n",
      "Epoch 15/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9999 - recall: 0.9481 - precision: 0.9478 - f1_score: 0.9467 - IoU: 0.9195\n",
      "Epoch 15: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 491ms/step - loss: 0.0656 - accuracy: 0.9999 - recall: 0.9481 - precision: 0.9478 - f1_score: 0.9467 - IoU: 0.9195 - val_loss: 0.9648 - val_accuracy: 0.9994 - val_recall: 0.0117 - val_precision: 1.0000 - val_f1_score: 0.0379 - val_IoU: 0.4999\n",
      "Epoch 16/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9999 - recall: 0.9515 - precision: 0.9523 - f1_score: 0.9498 - IoU: 0.9235\n",
      "Epoch 16: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 20s 488ms/step - loss: 0.0597 - accuracy: 0.9999 - recall: 0.9515 - precision: 0.9523 - f1_score: 0.9498 - IoU: 0.9235 - val_loss: 0.6996 - val_accuracy: 0.9995 - val_recall: 0.1660 - val_precision: 0.9916 - val_f1_score: 0.3247 - val_IoU: 0.5095\n",
      "Epoch 17/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9999 - recall: 0.9568 - precision: 0.9581 - f1_score: 0.9567 - IoU: 0.9294\n",
      "Epoch 17: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 492ms/step - loss: 0.0517 - accuracy: 0.9999 - recall: 0.9568 - precision: 0.9581 - f1_score: 0.9567 - IoU: 0.9294 - val_loss: 0.6390 - val_accuracy: 0.9995 - val_recall: 0.2055 - val_precision: 0.9911 - val_f1_score: 0.3923 - val_IoU: 0.5207\n",
      "Epoch 18/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9999 - recall: 0.9590 - precision: 0.9589 - f1_score: 0.9561 - IoU: 0.9305\n",
      "Epoch 18: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 490ms/step - loss: 0.0514 - accuracy: 0.9999 - recall: 0.9590 - precision: 0.9589 - f1_score: 0.9561 - IoU: 0.9305 - val_loss: 0.5253 - val_accuracy: 0.9996 - val_recall: 0.3027 - val_precision: 0.9940 - val_f1_score: 0.5035 - val_IoU: 0.5432\n",
      "Epoch 19/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9572 - precision: 0.9562 - f1_score: 0.9553 - IoU: 0.9319\n",
      "Epoch 19: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 512ms/step - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9572 - precision: 0.9562 - f1_score: 0.9553 - IoU: 0.9319 - val_loss: 0.3720 - val_accuracy: 0.9997 - val_recall: 0.4505 - val_precision: 0.9886 - val_f1_score: 0.6513 - val_IoU: 0.6045\n",
      "Epoch 20/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9599 - f1_score: 0.9595 - IoU: 0.9334\n",
      "Epoch 20: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 21s 492ms/step - loss: 0.0468 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9599 - f1_score: 0.9595 - IoU: 0.9334 - val_loss: 0.3101 - val_accuracy: 0.9997 - val_recall: 0.5393 - val_precision: 0.9933 - val_f1_score: 0.7123 - val_IoU: 0.6541\n",
      "Epoch 21/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9576 - precision: 0.9539 - f1_score: 0.9538 - IoU: 0.9302\n",
      "Epoch 21: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 29s 690ms/step - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9576 - precision: 0.9539 - f1_score: 0.9538 - IoU: 0.9302 - val_loss: 0.3643 - val_accuracy: 0.9995 - val_recall: 0.8675 - val_precision: 0.5569 - val_f1_score: 0.6173 - val_IoU: 0.7367\n",
      "Epoch 22/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9999 - recall: 0.9104 - precision: 0.9223 - f1_score: 0.9096 - IoU: 0.9010\n",
      "Epoch 22: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 31s 742ms/step - loss: 0.0953 - accuracy: 0.9999 - recall: 0.9104 - precision: 0.9223 - f1_score: 0.9096 - IoU: 0.9010 - val_loss: 0.7938 - val_accuracy: 0.9959 - val_recall: 0.8121 - val_precision: 0.1069 - val_f1_score: 0.2109 - val_IoU: 0.6151\n",
      "Epoch 23/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9999 - recall: 0.9257 - precision: 0.9362 - f1_score: 0.9293 - IoU: 0.9123\n",
      "Epoch 23: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 30s 709ms/step - loss: 0.0753 - accuracy: 0.9999 - recall: 0.9257 - precision: 0.9362 - f1_score: 0.9293 - IoU: 0.9123 - val_loss: 0.6641 - val_accuracy: 0.9979 - val_recall: 0.9510 - val_precision: 0.2080 - val_f1_score: 0.3298 - val_IoU: 0.6354\n",
      "Epoch 24/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9999 - recall: 0.9537 - precision: 0.9519 - f1_score: 0.9528 - IoU: 0.9280\n",
      "Epoch 24: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 28s 665ms/step - loss: 0.0514 - accuracy: 0.9999 - recall: 0.9537 - precision: 0.9519 - f1_score: 0.9528 - IoU: 0.9280 - val_loss: 0.3218 - val_accuracy: 0.9997 - val_recall: 0.5652 - val_precision: 0.9949 - val_f1_score: 0.6855 - val_IoU: 0.7067\n",
      "Epoch 25/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 1.0000 - recall: 0.9624 - precision: 0.9596 - f1_score: 0.9598 - IoU: 0.9377\n",
      "Epoch 25: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 31s 732ms/step - loss: 0.0440 - accuracy: 1.0000 - recall: 0.9624 - precision: 0.9596 - f1_score: 0.9598 - IoU: 0.9377 - val_loss: 0.2204 - val_accuracy: 0.9998 - val_recall: 0.6477 - val_precision: 0.9948 - val_f1_score: 0.7848 - val_IoU: 0.7027\n",
      "Epoch 26/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 1.0000 - recall: 0.9605 - precision: 0.9630 - f1_score: 0.9611 - IoU: 0.9357\n",
      "Epoch 26: val_f1_score did not improve from 0.83112\n",
      "42/42 [==============================] - 28s 668ms/step - loss: 0.0425 - accuracy: 1.0000 - recall: 0.9605 - precision: 0.9630 - f1_score: 0.9611 - IoU: 0.9357 - val_loss: 0.2473 - val_accuracy: 0.9998 - val_recall: 0.6037 - val_precision: 0.9949 - val_f1_score: 0.7542 - val_IoU: 0.6794\n",
      "Epoch 27/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0451 - accuracy: 0.9999 - recall: 0.9586 - precision: 0.9612 - f1_score: 0.9580 - IoU: 0.9350\n",
      "Epoch 27: val_f1_score improved from 0.83112 to 0.92608, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 28s 680ms/step - loss: 0.0451 - accuracy: 0.9999 - recall: 0.9586 - precision: 0.9612 - f1_score: 0.9580 - IoU: 0.9350 - val_loss: 0.0828 - val_accuracy: 0.9999 - val_recall: 0.8548 - val_precision: 0.9856 - val_f1_score: 0.9261 - val_IoU: 0.8213\n",
      "Epoch 28/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 1.0000 - recall: 0.9620 - precision: 0.9620 - f1_score: 0.9609 - IoU: 0.9374\n",
      "Epoch 28: val_f1_score improved from 0.92608 to 0.94472, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 31s 728ms/step - loss: 0.0423 - accuracy: 1.0000 - recall: 0.9620 - precision: 0.9620 - f1_score: 0.9609 - IoU: 0.9374 - val_loss: 0.0597 - val_accuracy: 0.9999 - val_recall: 0.9267 - val_precision: 0.9820 - val_f1_score: 0.9447 - val_IoU: 0.9157\n",
      "Epoch 29/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9651 - f1_score: 0.9640 - IoU: 0.9430\n",
      "Epoch 29: val_f1_score did not improve from 0.94472\n",
      "42/42 [==============================] - 29s 694ms/step - loss: 0.0388 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9651 - f1_score: 0.9640 - IoU: 0.9430 - val_loss: 0.1391 - val_accuracy: 0.9999 - val_recall: 0.8112 - val_precision: 0.9905 - val_f1_score: 0.8559 - val_IoU: 0.8405\n",
      "Epoch 30/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 1.0000 - recall: 0.9652 - precision: 0.9592 - f1_score: 0.9613 - IoU: 0.9428\n",
      "Epoch 30: val_f1_score did not improve from 0.94472\n",
      "42/42 [==============================] - 30s 714ms/step - loss: 0.0415 - accuracy: 1.0000 - recall: 0.9652 - precision: 0.9592 - f1_score: 0.9613 - IoU: 0.9428 - val_loss: 0.0607 - val_accuracy: 0.9999 - val_recall: 0.9124 - val_precision: 0.9929 - val_f1_score: 0.9418 - val_IoU: 0.8945\n",
      "Epoch 31/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000 - recall: 0.9648 - precision: 0.9629 - f1_score: 0.9635 - IoU: 0.9421\n",
      "Epoch 31: val_f1_score did not improve from 0.94472\n",
      "42/42 [==============================] - 27s 634ms/step - loss: 0.0389 - accuracy: 1.0000 - recall: 0.9648 - precision: 0.9629 - f1_score: 0.9635 - IoU: 0.9421 - val_loss: 0.0924 - val_accuracy: 0.9999 - val_recall: 0.8605 - val_precision: 0.9889 - val_f1_score: 0.9037 - val_IoU: 0.8498\n",
      "Epoch 32/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 1.0000 - recall: 0.9671 - precision: 0.9652 - f1_score: 0.9660 - IoU: 0.9445\n",
      "Epoch 32: val_f1_score improved from 0.94472 to 0.96409, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 0.0364 - accuracy: 1.0000 - recall: 0.9671 - precision: 0.9652 - f1_score: 0.9660 - IoU: 0.9445 - val_loss: 0.0427 - val_accuracy: 1.0000 - val_recall: 0.9606 - val_precision: 0.9781 - val_f1_score: 0.9641 - val_IoU: 0.9324\n",
      "Epoch 33/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9667 - f1_score: 0.9663 - IoU: 0.9445\n",
      "Epoch 33: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 24s 577ms/step - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9667 - f1_score: 0.9663 - IoU: 0.9445 - val_loss: 0.0682 - val_accuracy: 0.9999 - val_recall: 0.9033 - val_precision: 0.9904 - val_f1_score: 0.9266 - val_IoU: 0.8783\n",
      "Epoch 34/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9644 - precision: 0.9628 - f1_score: 0.9631 - IoU: 0.9446\n",
      "Epoch 34: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 29s 696ms/step - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9644 - precision: 0.9628 - f1_score: 0.9631 - IoU: 0.9446 - val_loss: 0.0864 - val_accuracy: 0.9999 - val_recall: 0.9608 - val_precision: 0.9346 - val_f1_score: 0.9067 - val_IoU: 0.9268\n",
      "Epoch 35/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 1.0000 - recall: 0.9659 - precision: 0.9607 - f1_score: 0.9615 - IoU: 0.9444\n",
      "Epoch 35: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 24s 574ms/step - loss: 0.0407 - accuracy: 1.0000 - recall: 0.9659 - precision: 0.9607 - f1_score: 0.9615 - IoU: 0.9444 - val_loss: 0.2507 - val_accuracy: 0.9998 - val_recall: 0.6177 - val_precision: 0.9820 - val_f1_score: 0.7585 - val_IoU: 0.7393\n",
      "Epoch 36/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 1.0000 - recall: 0.9617 - precision: 0.9605 - f1_score: 0.9597 - IoU: 0.9413\n",
      "Epoch 36: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 25s 594ms/step - loss: 0.0423 - accuracy: 1.0000 - recall: 0.9617 - precision: 0.9605 - f1_score: 0.9597 - IoU: 0.9413 - val_loss: 0.0913 - val_accuracy: 0.9999 - val_recall: 0.8639 - val_precision: 0.9908 - val_f1_score: 0.9119 - val_IoU: 0.8638\n",
      "Epoch 37/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9999 - recall: 0.9567 - precision: 0.9565 - f1_score: 0.9566 - IoU: 0.9384\n",
      "Epoch 37: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 24s 583ms/step - loss: 0.0453 - accuracy: 0.9999 - recall: 0.9567 - precision: 0.9565 - f1_score: 0.9566 - IoU: 0.9384 - val_loss: 0.1311 - val_accuracy: 0.9999 - val_recall: 0.7767 - val_precision: 0.9899 - val_f1_score: 0.8737 - val_IoU: 0.7920\n",
      "Epoch 38/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9999 - recall: 0.9321 - precision: 0.9352 - f1_score: 0.9310 - IoU: 0.9202\n",
      "Epoch 38: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0708 - accuracy: 0.9999 - recall: 0.9321 - precision: 0.9352 - f1_score: 0.9310 - IoU: 0.9202 - val_loss: 0.8088 - val_accuracy: 0.9995 - val_recall: 0.0855 - val_precision: 0.9918 - val_f1_score: 0.2121 - val_IoU: 0.5279\n",
      "Epoch 39/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9500 - precision: 0.9563 - f1_score: 0.9504 - IoU: 0.9376\n",
      "Epoch 39: val_f1_score did not improve from 0.96409\n",
      "42/42 [==============================] - 23s 552ms/step - loss: 0.0513 - accuracy: 0.9999 - recall: 0.9500 - precision: 0.9563 - f1_score: 0.9504 - IoU: 0.9376 - val_loss: 0.2272 - val_accuracy: 0.9997 - val_recall: 0.9465 - val_precision: 0.7038 - val_f1_score: 0.7598 - val_IoU: 0.8427\n",
      "Epoch 40/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 1.0000 - recall: 0.9623 - precision: 0.9641 - f1_score: 0.9623 - IoU: 0.9431\n",
      "Epoch 40: val_f1_score improved from 0.96409 to 0.96666, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 563ms/step - loss: 0.0393 - accuracy: 1.0000 - recall: 0.9623 - precision: 0.9641 - f1_score: 0.9623 - IoU: 0.9431 - val_loss: 0.0339 - val_accuracy: 1.0000 - val_recall: 0.9685 - val_precision: 0.9797 - val_f1_score: 0.9667 - val_IoU: 0.9470\n",
      "Epoch 41/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 1.0000 - recall: 0.9667 - precision: 0.9631 - f1_score: 0.9628 - IoU: 0.9472\n",
      "Epoch 41: val_f1_score improved from 0.96666 to 0.96888, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 28s 675ms/step - loss: 0.0386 - accuracy: 1.0000 - recall: 0.9667 - precision: 0.9631 - f1_score: 0.9628 - IoU: 0.9472 - val_loss: 0.0325 - val_accuracy: 1.0000 - val_recall: 0.9845 - val_precision: 0.9665 - val_f1_score: 0.9689 - val_IoU: 0.9609\n",
      "Epoch 42/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9683 - f1_score: 0.9690 - IoU: 0.9492\n",
      "Epoch 42: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 25s 601ms/step - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9683 - f1_score: 0.9690 - IoU: 0.9492 - val_loss: 0.0418 - val_accuracy: 1.0000 - val_recall: 0.9586 - val_precision: 0.9807 - val_f1_score: 0.9600 - val_IoU: 0.9413\n",
      "Epoch 43/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9708 - precision: 0.9664 - f1_score: 0.9689 - IoU: 0.9509\n",
      "Epoch 43: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 27s 645ms/step - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9708 - precision: 0.9664 - f1_score: 0.9689 - IoU: 0.9509 - val_loss: 0.0369 - val_accuracy: 1.0000 - val_recall: 0.9588 - val_precision: 0.9887 - val_f1_score: 0.9587 - val_IoU: 0.9409\n",
      "Epoch 44/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9699 - f1_score: 0.9676 - IoU: 0.9490\n",
      "Epoch 44: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 26s 608ms/step - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9699 - f1_score: 0.9676 - IoU: 0.9490 - val_loss: 0.0405 - val_accuracy: 1.0000 - val_recall: 0.9495 - val_precision: 0.9898 - val_f1_score: 0.9613 - val_IoU: 0.9366\n",
      "Epoch 45/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 1.0000 - recall: 0.9682 - precision: 0.9651 - f1_score: 0.9667 - IoU: 0.9495\n",
      "Epoch 45: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 25s 599ms/step - loss: 0.0345 - accuracy: 1.0000 - recall: 0.9682 - precision: 0.9651 - f1_score: 0.9667 - IoU: 0.9495 - val_loss: 0.0881 - val_accuracy: 0.9999 - val_recall: 0.9706 - val_precision: 0.9136 - val_f1_score: 0.9153 - val_IoU: 0.9287\n",
      "Epoch 46/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 1.0000 - recall: 0.9640 - precision: 0.9639 - f1_score: 0.9627 - IoU: 0.9462\n",
      "Epoch 46: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 26s 605ms/step - loss: 0.0388 - accuracy: 1.0000 - recall: 0.9640 - precision: 0.9639 - f1_score: 0.9627 - IoU: 0.9462 - val_loss: 0.1387 - val_accuracy: 0.9998 - val_recall: 0.7430 - val_precision: 0.9946 - val_f1_score: 0.8591 - val_IoU: 0.8077\n",
      "Epoch 47/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9648 - precision: 0.9666 - f1_score: 0.9647 - IoU: 0.9470\n",
      "Epoch 47: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9648 - precision: 0.9666 - f1_score: 0.9647 - IoU: 0.9470 - val_loss: 0.1662 - val_accuracy: 0.9998 - val_recall: 0.7563 - val_precision: 0.9807 - val_f1_score: 0.8242 - val_IoU: 0.8441\n",
      "Epoch 48/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000 - recall: 0.9661 - precision: 0.9683 - f1_score: 0.9661 - IoU: 0.9488\n",
      "Epoch 48: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 22s 532ms/step - loss: 0.0351 - accuracy: 1.0000 - recall: 0.9661 - precision: 0.9683 - f1_score: 0.9661 - IoU: 0.9488 - val_loss: 0.0930 - val_accuracy: 0.9999 - val_recall: 0.9919 - val_precision: 0.8971 - val_f1_score: 0.8979 - val_IoU: 0.9471\n",
      "Epoch 49/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9687 - f1_score: 0.9687 - IoU: 0.9528\n",
      "Epoch 49: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 23s 538ms/step - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9687 - f1_score: 0.9687 - IoU: 0.9528 - val_loss: 0.0372 - val_accuracy: 1.0000 - val_recall: 0.9593 - val_precision: 0.9839 - val_f1_score: 0.9644 - val_IoU: 0.9417\n",
      "Epoch 50/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000 - recall: 0.9680 - precision: 0.9690 - f1_score: 0.9678 - IoU: 0.9504\n",
      "Epoch 50: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 21s 498ms/step - loss: 0.0334 - accuracy: 1.0000 - recall: 0.9680 - precision: 0.9690 - f1_score: 0.9678 - IoU: 0.9504 - val_loss: 0.0698 - val_accuracy: 0.9999 - val_recall: 0.8778 - val_precision: 0.9917 - val_f1_score: 0.9334 - val_IoU: 0.8736\n",
      "Epoch 51/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9667 - f1_score: 0.9693 - IoU: 0.9534\n",
      "Epoch 51: val_f1_score did not improve from 0.96888\n",
      "42/42 [==============================] - 23s 561ms/step - loss: 0.0319 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9667 - f1_score: 0.9693 - IoU: 0.9534 - val_loss: 0.0349 - val_accuracy: 1.0000 - val_recall: 0.9619 - val_precision: 0.9867 - val_f1_score: 0.9661 - val_IoU: 0.9440\n",
      "Epoch 52/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9676 - f1_score: 0.9669 - IoU: 0.9501\n",
      "Epoch 52: val_f1_score improved from 0.96888 to 0.97662, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 23s 538ms/step - loss: 0.0342 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9676 - f1_score: 0.9669 - IoU: 0.9501 - val_loss: 0.0244 - val_accuracy: 1.0000 - val_recall: 0.9841 - val_precision: 0.9752 - val_f1_score: 0.9766 - val_IoU: 0.9631\n",
      "Epoch 53/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9682 - f1_score: 0.9701 - IoU: 0.9534\n",
      "Epoch 53: val_f1_score did not improve from 0.97662\n",
      "42/42 [==============================] - 25s 608ms/step - loss: 0.0309 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9682 - f1_score: 0.9701 - IoU: 0.9534 - val_loss: 0.0491 - val_accuracy: 1.0000 - val_recall: 0.9233 - val_precision: 0.9933 - val_f1_score: 0.9526 - val_IoU: 0.9286\n",
      "Epoch 54/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9686 - f1_score: 0.9713 - IoU: 0.9557\n",
      "Epoch 54: val_f1_score improved from 0.97662 to 0.97736, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 28s 673ms/step - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9686 - f1_score: 0.9713 - IoU: 0.9557 - val_loss: 0.0237 - val_accuracy: 1.0000 - val_recall: 0.9781 - val_precision: 0.9830 - val_f1_score: 0.9774 - val_IoU: 0.9580\n",
      "Epoch 55/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9703 - precision: 0.9702 - f1_score: 0.9694 - IoU: 0.9537\n",
      "Epoch 55: val_f1_score did not improve from 0.97736\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9703 - precision: 0.9702 - f1_score: 0.9694 - IoU: 0.9537 - val_loss: 0.0236 - val_accuracy: 1.0000 - val_recall: 0.9793 - val_precision: 0.9816 - val_f1_score: 0.9767 - val_IoU: 0.9588\n",
      "Epoch 56/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9747 - precision: 0.9701 - f1_score: 0.9693 - IoU: 0.9571\n",
      "Epoch 56: val_f1_score did not improve from 0.97736\n",
      "42/42 [==============================] - 28s 662ms/step - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9747 - precision: 0.9701 - f1_score: 0.9693 - IoU: 0.9571 - val_loss: 0.0310 - val_accuracy: 1.0000 - val_recall: 0.9592 - val_precision: 0.9894 - val_f1_score: 0.9695 - val_IoU: 0.9447\n",
      "Epoch 57/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9723 - precision: 0.9713 - f1_score: 0.9704 - IoU: 0.9557\n",
      "Epoch 57: val_f1_score did not improve from 0.97736\n",
      "42/42 [==============================] - 25s 599ms/step - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9723 - precision: 0.9713 - f1_score: 0.9704 - IoU: 0.9557 - val_loss: 0.0336 - val_accuracy: 1.0000 - val_recall: 0.9562 - val_precision: 0.9908 - val_f1_score: 0.9681 - val_IoU: 0.9444\n",
      "Epoch 58/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9691 - f1_score: 0.9692 - IoU: 0.9526\n",
      "Epoch 58: val_f1_score did not improve from 0.97736\n",
      "42/42 [==============================] - 24s 574ms/step - loss: 0.0317 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9691 - f1_score: 0.9692 - IoU: 0.9526 - val_loss: 0.0268 - val_accuracy: 1.0000 - val_recall: 0.9773 - val_precision: 0.9833 - val_f1_score: 0.9770 - val_IoU: 0.9574\n",
      "Epoch 59/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9716 - precision: 0.9725 - f1_score: 0.9692 - IoU: 0.9537\n",
      "Epoch 59: val_f1_score did not improve from 0.97736\n",
      "42/42 [==============================] - 24s 581ms/step - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9716 - precision: 0.9725 - f1_score: 0.9692 - IoU: 0.9537 - val_loss: 0.0265 - val_accuracy: 1.0000 - val_recall: 0.9625 - val_precision: 0.9904 - val_f1_score: 0.9733 - val_IoU: 0.9475\n",
      "Epoch 60/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9762 - precision: 0.9687 - f1_score: 0.9706 - IoU: 0.9579\n",
      "Epoch 60: val_f1_score improved from 0.97736 to 0.97843, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 26s 625ms/step - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9762 - precision: 0.9687 - f1_score: 0.9706 - IoU: 0.9579 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9814 - val_precision: 0.9827 - val_f1_score: 0.9784 - val_IoU: 0.9622\n",
      "Epoch 61/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000 - recall: 0.9735 - precision: 0.9712 - f1_score: 0.9717 - IoU: 0.9565\n",
      "Epoch 61: val_f1_score did not improve from 0.97843\n",
      "42/42 [==============================] - 25s 605ms/step - loss: 0.0291 - accuracy: 1.0000 - recall: 0.9735 - precision: 0.9712 - f1_score: 0.9717 - IoU: 0.9565 - val_loss: 0.0225 - val_accuracy: 1.0000 - val_recall: 0.9741 - val_precision: 0.9873 - val_f1_score: 0.9778 - val_IoU: 0.9565\n",
      "Epoch 62/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9717 - f1_score: 0.9696 - IoU: 0.9543\n",
      "Epoch 62: val_f1_score did not improve from 0.97843\n",
      "42/42 [==============================] - 25s 593ms/step - loss: 0.0312 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9717 - f1_score: 0.9696 - IoU: 0.9543 - val_loss: 0.0230 - val_accuracy: 1.0000 - val_recall: 0.9888 - val_precision: 0.9703 - val_f1_score: 0.9767 - val_IoU: 0.9710\n",
      "Epoch 63/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9708 - f1_score: 0.9725 - IoU: 0.9571\n",
      "Epoch 63: val_f1_score improved from 0.97843 to 0.97913, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 582ms/step - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9708 - f1_score: 0.9725 - IoU: 0.9571 - val_loss: 0.0212 - val_accuracy: 1.0000 - val_recall: 0.9814 - val_precision: 0.9813 - val_f1_score: 0.9791 - val_IoU: 0.9627\n",
      "Epoch 64/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9739 - precision: 0.9713 - f1_score: 0.9724 - IoU: 0.9569\n",
      "Epoch 64: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 22s 528ms/step - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9739 - precision: 0.9713 - f1_score: 0.9724 - IoU: 0.9569 - val_loss: 0.3316 - val_accuracy: 0.9997 - val_recall: 0.5710 - val_precision: 0.9931 - val_f1_score: 0.6821 - val_IoU: 0.7563\n",
      "Epoch 65/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000 - recall: 0.9687 - precision: 0.9710 - f1_score: 0.9687 - IoU: 0.9547\n",
      "Epoch 65: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 22s 522ms/step - loss: 0.0321 - accuracy: 1.0000 - recall: 0.9687 - precision: 0.9710 - f1_score: 0.9687 - IoU: 0.9547 - val_loss: 0.0508 - val_accuracy: 0.9999 - val_recall: 0.9131 - val_precision: 0.9910 - val_f1_score: 0.9504 - val_IoU: 0.9082\n",
      "Epoch 66/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9999 - recall: 0.9197 - precision: 0.9419 - f1_score: 0.9279 - IoU: 0.9227\n",
      "Epoch 66: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 21s 500ms/step - loss: 0.0731 - accuracy: 0.9999 - recall: 0.9197 - precision: 0.9419 - f1_score: 0.9279 - IoU: 0.9227 - val_loss: 0.7379 - val_accuracy: 0.9981 - val_recall: 0.5934 - val_precision: 0.1705 - val_f1_score: 0.2559 - val_IoU: 0.5730\n",
      "Epoch 67/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9999 - recall: 0.9532 - precision: 0.9563 - f1_score: 0.9540 - IoU: 0.9390\n",
      "Epoch 67: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 0.0469 - accuracy: 0.9999 - recall: 0.9532 - precision: 0.9563 - f1_score: 0.9540 - IoU: 0.9390 - val_loss: 0.5139 - val_accuracy: 0.9996 - val_recall: 0.2992 - val_precision: 0.9912 - val_f1_score: 0.5008 - val_IoU: 0.6195\n",
      "Epoch 68/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9999 - recall: 0.9594 - precision: 0.9622 - f1_score: 0.9599 - IoU: 0.9460\n",
      "Epoch 68: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 542ms/step - loss: 0.0408 - accuracy: 0.9999 - recall: 0.9594 - precision: 0.9622 - f1_score: 0.9599 - IoU: 0.9460 - val_loss: 0.0741 - val_accuracy: 0.9999 - val_recall: 0.9354 - val_precision: 0.9361 - val_f1_score: 0.9121 - val_IoU: 0.9402\n",
      "Epoch 69/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9999 - recall: 0.9562 - precision: 0.9590 - f1_score: 0.9566 - IoU: 0.9454\n",
      "Epoch 69: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 561ms/step - loss: 0.0441 - accuracy: 0.9999 - recall: 0.9562 - precision: 0.9590 - f1_score: 0.9566 - IoU: 0.9454 - val_loss: 0.0418 - val_accuracy: 1.0000 - val_recall: 0.9643 - val_precision: 0.9721 - val_f1_score: 0.9649 - val_IoU: 0.9533\n",
      "Epoch 70/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000 - recall: 0.9678 - precision: 0.9673 - f1_score: 0.9658 - IoU: 0.9531\n",
      "Epoch 70: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 583ms/step - loss: 0.0349 - accuracy: 1.0000 - recall: 0.9678 - precision: 0.9673 - f1_score: 0.9658 - IoU: 0.9531 - val_loss: 0.0755 - val_accuracy: 0.9999 - val_recall: 0.9083 - val_precision: 0.9844 - val_f1_score: 0.9287 - val_IoU: 0.9207\n",
      "Epoch 71/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9646 - f1_score: 0.9667 - IoU: 0.9550\n",
      "Epoch 71: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 552ms/step - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9646 - f1_score: 0.9667 - IoU: 0.9550 - val_loss: 0.0584 - val_accuracy: 0.9999 - val_recall: 0.9012 - val_precision: 0.9935 - val_f1_score: 0.9423 - val_IoU: 0.9045\n",
      "Epoch 72/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9577 - f1_score: 0.9583 - IoU: 0.9462\n",
      "Epoch 72: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 549ms/step - loss: 0.0423 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9577 - f1_score: 0.9583 - IoU: 0.9462 - val_loss: 0.9489 - val_accuracy: 0.9994 - val_recall: 0.0238 - val_precision: 0.9902 - val_f1_score: 0.0481 - val_IoU: 0.5060\n",
      "Epoch 73/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9999 - recall: 0.9528 - precision: 0.9593 - f1_score: 0.9554 - IoU: 0.9418\n",
      "Epoch 73: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0452 - accuracy: 0.9999 - recall: 0.9528 - precision: 0.9593 - f1_score: 0.9554 - IoU: 0.9418 - val_loss: 0.1502 - val_accuracy: 0.9999 - val_recall: 0.9235 - val_precision: 0.8629 - val_f1_score: 0.8391 - val_IoU: 0.8735\n",
      "Epoch 74/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000 - recall: 0.9675 - precision: 0.9655 - f1_score: 0.9663 - IoU: 0.9523\n",
      "Epoch 74: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 26s 632ms/step - loss: 0.0344 - accuracy: 1.0000 - recall: 0.9675 - precision: 0.9655 - f1_score: 0.9663 - IoU: 0.9523 - val_loss: 0.3186 - val_accuracy: 0.9996 - val_recall: 0.9290 - val_precision: 0.5812 - val_f1_score: 0.6617 - val_IoU: 0.7823\n",
      "Epoch 75/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9679 - f1_score: 0.9691 - IoU: 0.9540\n",
      "Epoch 75: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 555ms/step - loss: 0.0315 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9679 - f1_score: 0.9691 - IoU: 0.9540 - val_loss: 0.1834 - val_accuracy: 0.9998 - val_recall: 0.9478 - val_precision: 0.8051 - val_f1_score: 0.7975 - val_IoU: 0.8758\n",
      "Epoch 76/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9670 - precision: 0.9682 - f1_score: 0.9669 - IoU: 0.9518\n",
      "Epoch 76: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9670 - precision: 0.9682 - f1_score: 0.9669 - IoU: 0.9518 - val_loss: 0.0319 - val_accuracy: 1.0000 - val_recall: 0.9851 - val_precision: 0.9622 - val_f1_score: 0.9682 - val_IoU: 0.9687\n",
      "Epoch 77/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9732 - precision: 0.9673 - f1_score: 0.9704 - IoU: 0.9563\n",
      "Epoch 77: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 596ms/step - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9732 - precision: 0.9673 - f1_score: 0.9704 - IoU: 0.9563 - val_loss: 0.0239 - val_accuracy: 1.0000 - val_recall: 0.9772 - val_precision: 0.9836 - val_f1_score: 0.9760 - val_IoU: 0.9616\n",
      "Epoch 78/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000 - recall: 0.9746 - precision: 0.9675 - f1_score: 0.9685 - IoU: 0.9586\n",
      "Epoch 78: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 550ms/step - loss: 0.0320 - accuracy: 1.0000 - recall: 0.9746 - precision: 0.9675 - f1_score: 0.9685 - IoU: 0.9586 - val_loss: 0.0696 - val_accuracy: 0.9999 - val_recall: 0.8964 - val_precision: 0.9905 - val_f1_score: 0.9285 - val_IoU: 0.9145\n",
      "Epoch 79/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9718 - precision: 0.9737 - f1_score: 0.9722 - IoU: 0.9566\n",
      "Epoch 79: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9718 - precision: 0.9737 - f1_score: 0.9722 - IoU: 0.9566 - val_loss: 0.0291 - val_accuracy: 1.0000 - val_recall: 0.9954 - val_precision: 0.9501 - val_f1_score: 0.9700 - val_IoU: 0.9786\n",
      "Epoch 80/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9739 - precision: 0.9701 - f1_score: 0.9714 - IoU: 0.9597\n",
      "Epoch 80: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 576ms/step - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9739 - precision: 0.9701 - f1_score: 0.9714 - IoU: 0.9597 - val_loss: 0.0211 - val_accuracy: 1.0000 - val_recall: 0.9905 - val_precision: 0.9716 - val_f1_score: 0.9780 - val_IoU: 0.9724\n",
      "Epoch 81/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9729 - f1_score: 0.9708 - IoU: 0.9589\n",
      "Epoch 81: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9729 - f1_score: 0.9708 - IoU: 0.9589 - val_loss: 0.0227 - val_accuracy: 1.0000 - val_recall: 0.9761 - val_precision: 0.9856 - val_f1_score: 0.9767 - val_IoU: 0.9622\n",
      "Epoch 82/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9720 - f1_score: 0.9748 - IoU: 0.9610\n",
      "Epoch 82: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 598ms/step - loss: 0.0258 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9720 - f1_score: 0.9748 - IoU: 0.9610 - val_loss: 0.0340 - val_accuracy: 1.0000 - val_recall: 0.9470 - val_precision: 0.9930 - val_f1_score: 0.9665 - val_IoU: 0.9456\n",
      "Epoch 83/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9717 - f1_score: 0.9730 - IoU: 0.9602\n",
      "Epoch 83: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 554ms/step - loss: 0.0274 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9717 - f1_score: 0.9730 - IoU: 0.9602 - val_loss: 0.0250 - val_accuracy: 1.0000 - val_recall: 0.9691 - val_precision: 0.9873 - val_f1_score: 0.9747 - val_IoU: 0.9589\n",
      "Epoch 84/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9671 - f1_score: 0.9684 - IoU: 0.9575\n",
      "Epoch 84: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 608ms/step - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9671 - f1_score: 0.9684 - IoU: 0.9575 - val_loss: 0.0882 - val_accuracy: 0.9999 - val_recall: 0.8702 - val_precision: 0.9883 - val_f1_score: 0.9060 - val_IoU: 0.9077\n",
      "Epoch 85/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9687 - f1_score: 0.9696 - IoU: 0.9577\n",
      "Epoch 85: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 580ms/step - loss: 0.0308 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9687 - f1_score: 0.9696 - IoU: 0.9577 - val_loss: 0.1751 - val_accuracy: 0.9998 - val_recall: 0.6966 - val_precision: 0.9911 - val_f1_score: 0.8320 - val_IoU: 0.8190\n",
      "Epoch 86/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9710 - f1_score: 0.9699 - IoU: 0.9596\n",
      "Epoch 86: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 554ms/step - loss: 0.0306 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9710 - f1_score: 0.9699 - IoU: 0.9596 - val_loss: 0.0457 - val_accuracy: 0.9999 - val_recall: 0.9201 - val_precision: 0.9925 - val_f1_score: 0.9553 - val_IoU: 0.9249\n",
      "Epoch 87/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9714 - f1_score: 0.9716 - IoU: 0.9584\n",
      "Epoch 87: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9714 - f1_score: 0.9716 - IoU: 0.9584 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_recall: 0.9826 - val_precision: 0.9712 - val_f1_score: 0.9737 - val_IoU: 0.9629\n",
      "Epoch 88/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 1.0000 - recall: 0.9763 - precision: 0.9712 - f1_score: 0.9737 - IoU: 0.9609\n",
      "Epoch 88: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0267 - accuracy: 1.0000 - recall: 0.9763 - precision: 0.9712 - f1_score: 0.9737 - IoU: 0.9609 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9923 - val_precision: 0.9673 - val_f1_score: 0.9779 - val_IoU: 0.9759\n",
      "Epoch 89/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9731 - f1_score: 0.9721 - IoU: 0.9600\n",
      "Epoch 89: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 558ms/step - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9731 - f1_score: 0.9721 - IoU: 0.9600 - val_loss: 0.0294 - val_accuracy: 1.0000 - val_recall: 0.9536 - val_precision: 0.9921 - val_f1_score: 0.9711 - val_IoU: 0.9486\n",
      "Epoch 90/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9729 - f1_score: 0.9735 - IoU: 0.9612\n",
      "Epoch 90: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 606ms/step - loss: 0.0268 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9729 - f1_score: 0.9735 - IoU: 0.9612 - val_loss: 0.0224 - val_accuracy: 1.0000 - val_recall: 0.9937 - val_precision: 0.9652 - val_f1_score: 0.9772 - val_IoU: 0.9775\n",
      "Epoch 91/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9723 - f1_score: 0.9734 - IoU: 0.9612\n",
      "Epoch 91: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 554ms/step - loss: 0.0271 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9723 - f1_score: 0.9734 - IoU: 0.9612 - val_loss: 0.0317 - val_accuracy: 1.0000 - val_recall: 0.9508 - val_precision: 0.9923 - val_f1_score: 0.9689 - val_IoU: 0.9474\n",
      "Epoch 92/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9697 - f1_score: 0.9711 - IoU: 0.9608\n",
      "Epoch 92: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 607ms/step - loss: 0.0293 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9697 - f1_score: 0.9711 - IoU: 0.9608 - val_loss: 0.0248 - val_accuracy: 1.0000 - val_recall: 0.9688 - val_precision: 0.9890 - val_f1_score: 0.9753 - val_IoU: 0.9588\n",
      "Epoch 93/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9725 - f1_score: 0.9727 - IoU: 0.9611\n",
      "Epoch 93: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 23s 558ms/step - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9750 - precision: 0.9725 - f1_score: 0.9727 - IoU: 0.9611 - val_loss: 0.0314 - val_accuracy: 1.0000 - val_recall: 0.9915 - val_precision: 0.9437 - val_f1_score: 0.9688 - val_IoU: 0.9640\n",
      "Epoch 94/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000 - recall: 0.9754 - precision: 0.9710 - f1_score: 0.9728 - IoU: 0.9614\n",
      "Epoch 94: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 580ms/step - loss: 0.0276 - accuracy: 1.0000 - recall: 0.9754 - precision: 0.9710 - f1_score: 0.9728 - IoU: 0.9614 - val_loss: 0.0233 - val_accuracy: 1.0000 - val_recall: 0.9703 - val_precision: 0.9865 - val_f1_score: 0.9770 - val_IoU: 0.9593\n",
      "Epoch 95/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9735 - f1_score: 0.9744 - IoU: 0.9606\n",
      "Epoch 95: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 603ms/step - loss: 0.0260 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9735 - f1_score: 0.9744 - IoU: 0.9606 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9902 - val_precision: 0.9697 - val_f1_score: 0.9776 - val_IoU: 0.9736\n",
      "Epoch 96/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9782 - precision: 0.9722 - f1_score: 0.9741 - IoU: 0.9631\n",
      "Epoch 96: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 582ms/step - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9782 - precision: 0.9722 - f1_score: 0.9741 - IoU: 0.9631 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9760 - val_precision: 0.9862 - val_f1_score: 0.9786 - val_IoU: 0.9615\n",
      "Epoch 97/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9745 - precision: 0.9723 - f1_score: 0.9707 - IoU: 0.9612\n",
      "Epoch 97: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 21s 508ms/step - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9745 - precision: 0.9723 - f1_score: 0.9707 - IoU: 0.9612 - val_loss: 0.0538 - val_accuracy: 1.0000 - val_recall: 0.9225 - val_precision: 0.9931 - val_f1_score: 0.9494 - val_IoU: 0.9294\n",
      "Epoch 98/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9999 - recall: 0.9555 - precision: 0.9542 - f1_score: 0.9545 - IoU: 0.9447\n",
      "Epoch 98: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0459 - accuracy: 0.9999 - recall: 0.9555 - precision: 0.9542 - f1_score: 0.9545 - IoU: 0.9447 - val_loss: 0.8453 - val_accuracy: 0.9995 - val_recall: 0.0750 - val_precision: 0.9831 - val_f1_score: 0.1492 - val_IoU: 0.5320\n",
      "Epoch 99/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000 - recall: 0.9670 - precision: 0.9692 - f1_score: 0.9683 - IoU: 0.9556\n",
      "Epoch 99: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 588ms/step - loss: 0.0322 - accuracy: 1.0000 - recall: 0.9670 - precision: 0.9692 - f1_score: 0.9683 - IoU: 0.9556 - val_loss: 0.0692 - val_accuracy: 0.9999 - val_recall: 0.9981 - val_precision: 0.8859 - val_f1_score: 0.9303 - val_IoU: 0.9717\n",
      "Epoch 100/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9694 - f1_score: 0.9702 - IoU: 0.9591\n",
      "Epoch 100: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 601ms/step - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9694 - f1_score: 0.9702 - IoU: 0.9591 - val_loss: 0.0440 - val_accuracy: 1.0000 - val_recall: 0.9378 - val_precision: 0.9786 - val_f1_score: 0.9568 - val_IoU: 0.9423\n",
      "Epoch 101/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000 - recall: 0.9702 - precision: 0.9641 - f1_score: 0.9668 - IoU: 0.9577\n",
      "Epoch 101: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 30s 709ms/step - loss: 0.0337 - accuracy: 1.0000 - recall: 0.9702 - precision: 0.9641 - f1_score: 0.9668 - IoU: 0.9577 - val_loss: 0.4642 - val_accuracy: 0.9997 - val_recall: 0.4528 - val_precision: 0.9905 - val_f1_score: 0.5233 - val_IoU: 0.7084\n",
      "Epoch 102/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9999 - recall: 0.9495 - precision: 0.9550 - f1_score: 0.9499 - IoU: 0.9426\n",
      "Epoch 102: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.0505 - accuracy: 0.9999 - recall: 0.9495 - precision: 0.9550 - f1_score: 0.9499 - IoU: 0.9426 - val_loss: 0.2628 - val_accuracy: 0.9998 - val_recall: 0.5791 - val_precision: 0.9944 - val_f1_score: 0.7456 - val_IoU: 0.7038\n",
      "Epoch 103/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9571 - f1_score: 0.9584 - IoU: 0.9490\n",
      "Epoch 103: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 26s 620ms/step - loss: 0.0420 - accuracy: 0.9999 - recall: 0.9610 - precision: 0.9571 - f1_score: 0.9584 - IoU: 0.9490 - val_loss: 0.0461 - val_accuracy: 1.0000 - val_recall: 0.9707 - val_precision: 0.9464 - val_f1_score: 0.9546 - val_IoU: 0.9603\n",
      "Epoch 104/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000 - recall: 0.9674 - precision: 0.9656 - f1_score: 0.9645 - IoU: 0.9556\n",
      "Epoch 104: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 576ms/step - loss: 0.0360 - accuracy: 1.0000 - recall: 0.9674 - precision: 0.9656 - f1_score: 0.9645 - IoU: 0.9556 - val_loss: 0.0419 - val_accuracy: 1.0000 - val_recall: 0.9591 - val_precision: 0.9692 - val_f1_score: 0.9570 - val_IoU: 0.9444\n",
      "Epoch 105/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9697 - f1_score: 0.9711 - IoU: 0.9573\n",
      "Epoch 105: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 596ms/step - loss: 0.0293 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9697 - f1_score: 0.9711 - IoU: 0.9573 - val_loss: 0.0509 - val_accuracy: 1.0000 - val_recall: 0.9266 - val_precision: 0.9937 - val_f1_score: 0.9468 - val_IoU: 0.9319\n",
      "Epoch 106/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9703 - f1_score: 0.9688 - IoU: 0.9568\n",
      "Epoch 106: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 25s 585ms/step - loss: 0.0316 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9703 - f1_score: 0.9688 - IoU: 0.9568 - val_loss: 0.0523 - val_accuracy: 0.9999 - val_recall: 0.9196 - val_precision: 0.9938 - val_f1_score: 0.9487 - val_IoU: 0.9294\n",
      "Epoch 107/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9768 - precision: 0.9698 - f1_score: 0.9727 - IoU: 0.9619\n",
      "Epoch 107: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 576ms/step - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9768 - precision: 0.9698 - f1_score: 0.9727 - IoU: 0.9619 - val_loss: 0.0390 - val_accuracy: 1.0000 - val_recall: 0.9397 - val_precision: 0.9938 - val_f1_score: 0.9610 - val_IoU: 0.9413\n",
      "Epoch 108/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9734 - f1_score: 0.9740 - IoU: 0.9614\n",
      "Epoch 108: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 579ms/step - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9734 - f1_score: 0.9740 - IoU: 0.9614 - val_loss: 0.0208 - val_accuracy: 1.0000 - val_recall: 0.9814 - val_precision: 0.9829 - val_f1_score: 0.9791 - val_IoU: 0.9655\n",
      "Epoch 109/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 1.0000 - recall: 0.9754 - precision: 0.9714 - f1_score: 0.9728 - IoU: 0.9622\n",
      "Epoch 109: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 562ms/step - loss: 0.0275 - accuracy: 1.0000 - recall: 0.9754 - precision: 0.9714 - f1_score: 0.9728 - IoU: 0.9622 - val_loss: 0.0296 - val_accuracy: 1.0000 - val_recall: 0.9565 - val_precision: 0.9919 - val_f1_score: 0.9707 - val_IoU: 0.9524\n",
      "Epoch 110/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 1.0000 - recall: 0.9752 - precision: 0.9730 - f1_score: 0.9742 - IoU: 0.9630\n",
      "Epoch 110: val_f1_score did not improve from 0.97913\n",
      "42/42 [==============================] - 24s 570ms/step - loss: 0.0260 - accuracy: 1.0000 - recall: 0.9752 - precision: 0.9730 - f1_score: 0.9742 - IoU: 0.9630 - val_loss: 0.0310 - val_accuracy: 1.0000 - val_recall: 0.9543 - val_precision: 0.9925 - val_f1_score: 0.9695 - val_IoU: 0.9521\n",
      "Epoch 111/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9738 - f1_score: 0.9736 - IoU: 0.9624\n",
      "Epoch 111: val_f1_score improved from 0.97913 to 0.97923, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 25s 603ms/step - loss: 0.0266 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9738 - f1_score: 0.9736 - IoU: 0.9624 - val_loss: 0.0208 - val_accuracy: 1.0000 - val_recall: 0.9761 - val_precision: 0.9883 - val_f1_score: 0.9792 - val_IoU: 0.9630\n",
      "Epoch 112/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 1.0000 - recall: 0.9774 - precision: 0.9723 - f1_score: 0.9752 - IoU: 0.9639\n",
      "Epoch 112: val_f1_score did not improve from 0.97923\n",
      "42/42 [==============================] - 26s 620ms/step - loss: 0.0252 - accuracy: 1.0000 - recall: 0.9774 - precision: 0.9723 - f1_score: 0.9752 - IoU: 0.9639 - val_loss: 0.0275 - val_accuracy: 1.0000 - val_recall: 0.9617 - val_precision: 0.9909 - val_f1_score: 0.9726 - val_IoU: 0.9560\n",
      "Epoch 113/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 1.0000 - recall: 0.9775 - precision: 0.9741 - f1_score: 0.9757 - IoU: 0.9638\n",
      "Epoch 113: val_f1_score improved from 0.97923 to 0.98004, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 26s 613ms/step - loss: 0.0246 - accuracy: 1.0000 - recall: 0.9775 - precision: 0.9741 - f1_score: 0.9757 - IoU: 0.9638 - val_loss: 0.0194 - val_accuracy: 1.0000 - val_recall: 0.9923 - val_precision: 0.9737 - val_f1_score: 0.9800 - val_IoU: 0.9769\n",
      "Epoch 114/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9717 - f1_score: 0.9732 - IoU: 0.9636\n",
      "Epoch 114: val_f1_score improved from 0.98004 to 0.98102, saving model to Trans UNet DICE 5-fold model/model_4fold.keras\n",
      "42/42 [==============================] - 24s 575ms/step - loss: 0.0270 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9717 - f1_score: 0.9732 - IoU: 0.9636 - val_loss: 0.0189 - val_accuracy: 1.0000 - val_recall: 0.9850 - val_precision: 0.9829 - val_f1_score: 0.9810 - val_IoU: 0.9700\n",
      "Epoch 115/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9756 - precision: 0.9763 - f1_score: 0.9749 - IoU: 0.9637\n",
      "Epoch 115: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 570ms/step - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9756 - precision: 0.9763 - f1_score: 0.9749 - IoU: 0.9637 - val_loss: 0.0217 - val_accuracy: 1.0000 - val_recall: 0.9727 - val_precision: 0.9889 - val_f1_score: 0.9784 - val_IoU: 0.9628\n",
      "Epoch 116/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 1.0000 - recall: 0.9785 - precision: 0.9721 - f1_score: 0.9742 - IoU: 0.9649\n",
      "Epoch 116: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 575ms/step - loss: 0.0262 - accuracy: 1.0000 - recall: 0.9785 - precision: 0.9721 - f1_score: 0.9742 - IoU: 0.9649 - val_loss: 0.0293 - val_accuracy: 1.0000 - val_recall: 0.9554 - val_precision: 0.9923 - val_f1_score: 0.9710 - val_IoU: 0.9542\n",
      "Epoch 117/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000 - recall: 0.9773 - precision: 0.9741 - f1_score: 0.9746 - IoU: 0.9642\n",
      "Epoch 117: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 596ms/step - loss: 0.0257 - accuracy: 1.0000 - recall: 0.9773 - precision: 0.9741 - f1_score: 0.9746 - IoU: 0.9642 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_recall: 0.9627 - val_precision: 0.9916 - val_f1_score: 0.9737 - val_IoU: 0.9568\n",
      "Epoch 118/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 1.0000 - recall: 0.9787 - precision: 0.9754 - f1_score: 0.9774 - IoU: 0.9654\n",
      "Epoch 118: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0229 - accuracy: 1.0000 - recall: 0.9787 - precision: 0.9754 - f1_score: 0.9774 - IoU: 0.9654 - val_loss: 0.0192 - val_accuracy: 1.0000 - val_recall: 0.9819 - val_precision: 0.9843 - val_f1_score: 0.9801 - val_IoU: 0.9679\n",
      "Epoch 119/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000 - recall: 0.9767 - precision: 0.9749 - f1_score: 0.9742 - IoU: 0.9637\n",
      "Epoch 119: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 0.0261 - accuracy: 1.0000 - recall: 0.9767 - precision: 0.9749 - f1_score: 0.9742 - IoU: 0.9637 - val_loss: 0.0187 - val_accuracy: 1.0000 - val_recall: 0.9882 - val_precision: 0.9790 - val_f1_score: 0.9810 - val_IoU: 0.9746\n",
      "Epoch 120/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 1.0000 - recall: 0.9788 - precision: 0.9739 - f1_score: 0.9754 - IoU: 0.9650\n",
      "Epoch 120: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 620ms/step - loss: 0.0250 - accuracy: 1.0000 - recall: 0.9788 - precision: 0.9739 - f1_score: 0.9754 - IoU: 0.9650 - val_loss: 0.0192 - val_accuracy: 1.0000 - val_recall: 0.9912 - val_precision: 0.9741 - val_f1_score: 0.9802 - val_IoU: 0.9761\n",
      "Epoch 121/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000 - recall: 0.9782 - precision: 0.9736 - f1_score: 0.9766 - IoU: 0.9648\n",
      "Epoch 121: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 617ms/step - loss: 0.0237 - accuracy: 1.0000 - recall: 0.9782 - precision: 0.9736 - f1_score: 0.9766 - IoU: 0.9648 - val_loss: 0.0244 - val_accuracy: 1.0000 - val_recall: 0.9716 - val_precision: 0.9886 - val_f1_score: 0.9751 - val_IoU: 0.9614\n",
      "Epoch 122/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9767 - f1_score: 0.9718 - IoU: 0.9616\n",
      "Epoch 122: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 27s 638ms/step - loss: 0.0284 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9767 - f1_score: 0.9718 - IoU: 0.9616 - val_loss: 0.0251 - val_accuracy: 1.0000 - val_recall: 0.9875 - val_precision: 0.9654 - val_f1_score: 0.9746 - val_IoU: 0.9743\n",
      "Epoch 123/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000 - recall: 0.9790 - precision: 0.9715 - f1_score: 0.9750 - IoU: 0.9648\n",
      "Epoch 123: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 585ms/step - loss: 0.0253 - accuracy: 1.0000 - recall: 0.9790 - precision: 0.9715 - f1_score: 0.9750 - IoU: 0.9648 - val_loss: 0.0255 - val_accuracy: 1.0000 - val_recall: 0.9672 - val_precision: 0.9910 - val_f1_score: 0.9743 - val_IoU: 0.9581\n",
      "Epoch 124/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 1.0000 - recall: 0.9770 - precision: 0.9754 - f1_score: 0.9755 - IoU: 0.9642\n",
      "Epoch 124: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 609ms/step - loss: 0.0248 - accuracy: 1.0000 - recall: 0.9770 - precision: 0.9754 - f1_score: 0.9755 - IoU: 0.9642 - val_loss: 0.0209 - val_accuracy: 1.0000 - val_recall: 0.9759 - val_precision: 0.9883 - val_f1_score: 0.9786 - val_IoU: 0.9655\n",
      "Epoch 125/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000 - recall: 0.9795 - precision: 0.9738 - f1_score: 0.9756 - IoU: 0.9663\n",
      "Epoch 125: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0247 - accuracy: 1.0000 - recall: 0.9795 - precision: 0.9738 - f1_score: 0.9756 - IoU: 0.9663 - val_loss: 0.0206 - val_accuracy: 1.0000 - val_recall: 0.9799 - val_precision: 0.9857 - val_f1_score: 0.9790 - val_IoU: 0.9686\n",
      "Epoch 126/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 1.0000 - recall: 0.9783 - precision: 0.9753 - f1_score: 0.9770 - IoU: 0.9659\n",
      "Epoch 126: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 611ms/step - loss: 0.0232 - accuracy: 1.0000 - recall: 0.9783 - precision: 0.9753 - f1_score: 0.9770 - IoU: 0.9659 - val_loss: 0.0228 - val_accuracy: 1.0000 - val_recall: 0.9727 - val_precision: 0.9888 - val_f1_score: 0.9774 - val_IoU: 0.9628\n",
      "Epoch 127/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 1.0000 - recall: 0.9773 - precision: 0.9752 - f1_score: 0.9761 - IoU: 0.9658\n",
      "Epoch 127: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0242 - accuracy: 1.0000 - recall: 0.9773 - precision: 0.9752 - f1_score: 0.9761 - IoU: 0.9658 - val_loss: 0.0242 - val_accuracy: 1.0000 - val_recall: 0.9704 - val_precision: 0.9903 - val_f1_score: 0.9759 - val_IoU: 0.9626\n",
      "Epoch 128/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9771 - precision: 0.9738 - f1_score: 0.9748 - IoU: 0.9658\n",
      "Epoch 128: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 592ms/step - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9771 - precision: 0.9738 - f1_score: 0.9748 - IoU: 0.9658 - val_loss: 0.0300 - val_accuracy: 1.0000 - val_recall: 0.9583 - val_precision: 0.9926 - val_f1_score: 0.9690 - val_IoU: 0.9568\n",
      "Epoch 129/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000 - recall: 0.9800 - precision: 0.9753 - f1_score: 0.9776 - IoU: 0.9669\n",
      "Epoch 129: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 602ms/step - loss: 0.0227 - accuracy: 1.0000 - recall: 0.9800 - precision: 0.9753 - f1_score: 0.9776 - IoU: 0.9669 - val_loss: 0.0199 - val_accuracy: 1.0000 - val_recall: 0.9766 - val_precision: 0.9883 - val_f1_score: 0.9803 - val_IoU: 0.9672\n",
      "Epoch 130/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 1.0000 - recall: 0.9806 - precision: 0.9737 - f1_score: 0.9768 - IoU: 0.9671\n",
      "Epoch 130: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 633ms/step - loss: 0.0233 - accuracy: 1.0000 - recall: 0.9806 - precision: 0.9737 - f1_score: 0.9768 - IoU: 0.9671 - val_loss: 0.0265 - val_accuracy: 1.0000 - val_recall: 0.9594 - val_precision: 0.9917 - val_f1_score: 0.9721 - val_IoU: 0.9577\n",
      "Epoch 131/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9790 - precision: 0.9750 - f1_score: 0.9749 - IoU: 0.9658\n",
      "Epoch 131: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 594ms/step - loss: 0.0254 - accuracy: 1.0000 - recall: 0.9790 - precision: 0.9750 - f1_score: 0.9749 - IoU: 0.9658 - val_loss: 0.0198 - val_accuracy: 1.0000 - val_recall: 0.9788 - val_precision: 0.9875 - val_f1_score: 0.9802 - val_IoU: 0.9679\n",
      "Epoch 132/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 1.0000 - recall: 0.9787 - precision: 0.9751 - f1_score: 0.9766 - IoU: 0.9673\n",
      "Epoch 132: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 616ms/step - loss: 0.0237 - accuracy: 1.0000 - recall: 0.9787 - precision: 0.9751 - f1_score: 0.9766 - IoU: 0.9673 - val_loss: 0.0225 - val_accuracy: 1.0000 - val_recall: 0.9702 - val_precision: 0.9904 - val_f1_score: 0.9771 - val_IoU: 0.9635\n",
      "Epoch 133/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 1.0000 - recall: 0.9776 - precision: 0.9763 - f1_score: 0.9768 - IoU: 0.9654\n",
      "Epoch 133: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 27s 637ms/step - loss: 0.0235 - accuracy: 1.0000 - recall: 0.9776 - precision: 0.9763 - f1_score: 0.9768 - IoU: 0.9654 - val_loss: 0.0225 - val_accuracy: 1.0000 - val_recall: 0.9835 - val_precision: 0.9785 - val_f1_score: 0.9764 - val_IoU: 0.9742\n",
      "Epoch 134/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9667 - f1_score: 0.9715 - IoU: 0.9616\n",
      "Epoch 134: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 27s 650ms/step - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9667 - f1_score: 0.9715 - IoU: 0.9616 - val_loss: 0.1473 - val_accuracy: 0.9998 - val_recall: 0.9969 - val_precision: 0.7944 - val_f1_score: 0.8455 - val_IoU: 0.9155\n",
      "Epoch 135/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9710 - f1_score: 0.9698 - IoU: 0.9603\n",
      "Epoch 135: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 27s 648ms/step - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9710 - precision: 0.9710 - f1_score: 0.9698 - IoU: 0.9603 - val_loss: 0.0642 - val_accuracy: 1.0000 - val_recall: 0.9547 - val_precision: 0.9632 - val_f1_score: 0.9351 - val_IoU: 0.9480\n",
      "Epoch 136/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9701 - f1_score: 0.9714 - IoU: 0.9622\n",
      "Epoch 136: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 29s 703ms/step - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9701 - f1_score: 0.9714 - IoU: 0.9622 - val_loss: 0.1197 - val_accuracy: 0.9999 - val_recall: 0.8181 - val_precision: 0.9328 - val_f1_score: 0.8831 - val_IoU: 0.8914\n",
      "Epoch 137/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 1.0000 - recall: 0.9718 - precision: 0.9668 - f1_score: 0.9680 - IoU: 0.9617\n",
      "Epoch 137: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 27s 644ms/step - loss: 0.0322 - accuracy: 1.0000 - recall: 0.9718 - precision: 0.9668 - f1_score: 0.9680 - IoU: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9999 - val_recall: 0.9882 - val_precision: 0.8190 - val_f1_score: 0.8431 - val_IoU: 0.9092\n",
      "Epoch 138/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9766 - precision: 0.9669 - f1_score: 0.9703 - IoU: 0.9632\n",
      "Epoch 138: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9766 - precision: 0.9669 - f1_score: 0.9703 - IoU: 0.9632 - val_loss: 0.0252 - val_accuracy: 1.0000 - val_recall: 0.9897 - val_precision: 0.9696 - val_f1_score: 0.9749 - val_IoU: 0.9760\n",
      "Epoch 139/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9669 - precision: 0.9673 - f1_score: 0.9644 - IoU: 0.9570\n",
      "Epoch 139: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 573ms/step - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9669 - precision: 0.9673 - f1_score: 0.9644 - IoU: 0.9570 - val_loss: 0.1242 - val_accuracy: 0.9999 - val_recall: 0.8126 - val_precision: 0.9765 - val_f1_score: 0.8807 - val_IoU: 0.8807\n",
      "Epoch 140/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9660 - precision: 0.9712 - f1_score: 0.9666 - IoU: 0.9585\n",
      "Epoch 140: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 584ms/step - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9660 - precision: 0.9712 - f1_score: 0.9666 - IoU: 0.9585 - val_loss: 0.1690 - val_accuracy: 0.9998 - val_recall: 0.9871 - val_precision: 0.7248 - val_f1_score: 0.8186 - val_IoU: 0.8684\n",
      "Epoch 141/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9710 - f1_score: 0.9712 - IoU: 0.9602\n",
      "Epoch 141: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 625ms/step - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9710 - f1_score: 0.9712 - IoU: 0.9602 - val_loss: 0.0372 - val_accuracy: 1.0000 - val_recall: 0.9619 - val_precision: 0.9769 - val_f1_score: 0.9624 - val_IoU: 0.9605\n",
      "Epoch 142/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9722 - f1_score: 0.9753 - IoU: 0.9653\n",
      "Epoch 142: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 561ms/step - loss: 0.0248 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9722 - f1_score: 0.9753 - IoU: 0.9653 - val_loss: 0.0419 - val_accuracy: 1.0000 - val_recall: 0.9377 - val_precision: 0.9918 - val_f1_score: 0.9596 - val_IoU: 0.9451\n",
      "Epoch 143/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 1.0000 - recall: 0.9757 - precision: 0.9736 - f1_score: 0.9744 - IoU: 0.9649\n",
      "Epoch 143: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 23s 556ms/step - loss: 0.0259 - accuracy: 1.0000 - recall: 0.9757 - precision: 0.9736 - f1_score: 0.9744 - IoU: 0.9649 - val_loss: 0.0306 - val_accuracy: 1.0000 - val_recall: 0.9660 - val_precision: 0.9864 - val_f1_score: 0.9697 - val_IoU: 0.9580\n",
      "Epoch 144/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9730 - f1_score: 0.9747 - IoU: 0.9648\n",
      "Epoch 144: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 564ms/step - loss: 0.0255 - accuracy: 1.0000 - recall: 0.9777 - precision: 0.9730 - f1_score: 0.9747 - IoU: 0.9648 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_recall: 0.9693 - val_precision: 0.9879 - val_f1_score: 0.9734 - val_IoU: 0.9605\n",
      "Epoch 145/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000 - recall: 0.9772 - precision: 0.9747 - f1_score: 0.9746 - IoU: 0.9648\n",
      "Epoch 145: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 561ms/step - loss: 0.0257 - accuracy: 1.0000 - recall: 0.9772 - precision: 0.9747 - f1_score: 0.9746 - IoU: 0.9648 - val_loss: 0.0336 - val_accuracy: 1.0000 - val_recall: 0.9632 - val_precision: 0.9827 - val_f1_score: 0.9665 - val_IoU: 0.9560\n",
      "Epoch 146/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9734 - f1_score: 0.9749 - IoU: 0.9650\n",
      "Epoch 146: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 609ms/step - loss: 0.0253 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9734 - f1_score: 0.9749 - IoU: 0.9650 - val_loss: 0.0410 - val_accuracy: 1.0000 - val_recall: 0.9702 - val_precision: 0.9676 - val_f1_score: 0.9597 - val_IoU: 0.9561\n",
      "Epoch 147/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9653 - precision: 0.9714 - f1_score: 0.9676 - IoU: 0.9590\n",
      "Epoch 147: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 26s 621ms/step - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9653 - precision: 0.9714 - f1_score: 0.9676 - IoU: 0.9590 - val_loss: 0.0354 - val_accuracy: 1.0000 - val_recall: 0.9953 - val_precision: 0.9384 - val_f1_score: 0.9645 - val_IoU: 0.9811\n",
      "Epoch 148/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9677 - f1_score: 0.9711 - IoU: 0.9623\n",
      "Epoch 148: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 25s 598ms/step - loss: 0.0292 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9677 - f1_score: 0.9711 - IoU: 0.9623 - val_loss: 0.0267 - val_accuracy: 1.0000 - val_recall: 0.9766 - val_precision: 0.9782 - val_f1_score: 0.9738 - val_IoU: 0.9698\n",
      "Epoch 149/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9722 - precision: 0.9727 - f1_score: 0.9708 - IoU: 0.9631\n",
      "Epoch 149: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 568ms/step - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9722 - precision: 0.9727 - f1_score: 0.9708 - IoU: 0.9631 - val_loss: 0.0531 - val_accuracy: 1.0000 - val_recall: 0.9865 - val_precision: 0.9522 - val_f1_score: 0.9475 - val_IoU: 0.9662\n",
      "Epoch 150/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000 - recall: 0.9763 - precision: 0.9741 - f1_score: 0.9755 - IoU: 0.9654\n",
      "Epoch 150: val_f1_score did not improve from 0.98102\n",
      "42/42 [==============================] - 24s 567ms/step - loss: 0.0247 - accuracy: 1.0000 - recall: 0.9763 - precision: 0.9741 - f1_score: 0.9755 - IoU: 0.9654 - val_loss: 0.0519 - val_accuracy: 1.0000 - val_recall: 0.9643 - val_precision: 0.9684 - val_f1_score: 0.9495 - val_IoU: 0.9493\n",
      "O modelo demorou 3826.85 segundos para treinar.\n",
      "Fold: 5\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 00:33:15.272228: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - ETA: 0s - loss: 0.9973 - accuracy: 0.6799 - recall: 0.9903 - precision: 0.0020 - f1_score: 0.0043 - IoU: 0.5046\n",
      "Epoch 1: val_f1_score improved from -inf to 0.00168, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 31s 518ms/step - loss: 0.9973 - accuracy: 0.6799 - recall: 0.9903 - precision: 0.0020 - f1_score: 0.0043 - IoU: 0.5046 - val_loss: 0.9983 - val_accuracy: 0.3327 - val_recall: 1.0000 - val_precision: 8.9272e-04 - val_f1_score: 0.0017 - val_IoU: 0.2018\n",
      "Epoch 2/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9952 - accuracy: 0.9656 - recall: 0.9931 - precision: 0.0180 - f1_score: 0.0515 - IoU: 0.5387\n",
      "Epoch 2: val_f1_score improved from 0.00168 to 0.04665, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 39s 933ms/step - loss: 0.9952 - accuracy: 0.9656 - recall: 0.9931 - precision: 0.0180 - f1_score: 0.0515 - IoU: 0.5387 - val_loss: 0.9792 - val_accuracy: 0.9770 - val_recall: 0.9976 - val_precision: 0.0252 - val_f1_score: 0.0467 - val_IoU: 0.5373\n",
      "Epoch 3/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9928 - accuracy: 0.9928 - recall: 0.9947 - precision: 0.0807 - f1_score: 0.1614 - IoU: 0.6118\n",
      "Epoch 3: val_f1_score did not improve from 0.04665\n",
      "42/42 [==============================] - 29s 687ms/step - loss: 0.9928 - accuracy: 0.9928 - recall: 0.9947 - precision: 0.0807 - f1_score: 0.1614 - IoU: 0.6118 - val_loss: 0.9996 - val_accuracy: 0.9633 - val_recall: 0.0221 - val_precision: 3.6425e-04 - val_f1_score: 6.9520e-04 - val_IoU: 0.4998\n",
      "Epoch 4/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9891 - accuracy: 0.9950 - recall: 0.9918 - precision: 0.1119 - f1_score: 0.2197 - IoU: 0.6291\n",
      "Epoch 4: val_f1_score did not improve from 0.04665\n",
      "42/42 [==============================] - 27s 633ms/step - loss: 0.9891 - accuracy: 0.9950 - recall: 0.9918 - precision: 0.1119 - f1_score: 0.2197 - IoU: 0.6291 - val_loss: 0.9988 - val_accuracy: 0.5213 - val_recall: 0.3375 - val_precision: 4.2047e-04 - val_f1_score: 8.2770e-04 - val_IoU: 0.5773\n",
      "Epoch 5/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9818 - accuracy: 0.9959 - recall: 0.9807 - precision: 0.1333 - f1_score: 0.2561 - IoU: 0.6428\n",
      "Epoch 5: val_f1_score did not improve from 0.04665\n",
      "42/42 [==============================] - 29s 692ms/step - loss: 0.9818 - accuracy: 0.9959 - recall: 0.9807 - precision: 0.1333 - f1_score: 0.2561 - IoU: 0.6428 - val_loss: 0.9939 - val_accuracy: 0.9000 - val_recall: 0.8988 - val_precision: 0.0053 - val_f1_score: 0.0103 - val_IoU: 0.6850\n",
      "Epoch 6/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9624 - accuracy: 0.9976 - recall: 0.9756 - precision: 0.2075 - f1_score: 0.3796 - IoU: 0.6791\n",
      "Epoch 6: val_f1_score did not improve from 0.04665\n",
      "42/42 [==============================] - 30s 729ms/step - loss: 0.9624 - accuracy: 0.9976 - recall: 0.9756 - precision: 0.2075 - f1_score: 0.3796 - IoU: 0.6791 - val_loss: 0.9962 - val_accuracy: 0.9994 - val_recall: 0.0148 - val_precision: 0.7914 - val_f1_score: 0.0346 - val_IoU: 0.5023\n",
      "Epoch 7/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9010 - accuracy: 0.9989 - recall: 0.9528 - precision: 0.3626 - f1_score: 0.5499 - IoU: 0.7394\n",
      "Epoch 7: val_f1_score did not improve from 0.04665\n",
      "42/42 [==============================] - 31s 733ms/step - loss: 0.9010 - accuracy: 0.9989 - recall: 0.9528 - precision: 0.3626 - f1_score: 0.5499 - IoU: 0.7394 - val_loss: 0.9998 - val_accuracy: 0.9994 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - val_f1_score: 0.0000e+00 - val_IoU: 0.4997\n",
      "Epoch 8/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7301 - accuracy: 0.9995 - recall: 0.9156 - precision: 0.5630 - f1_score: 0.7041 - IoU: 0.8104\n",
      "Epoch 8: val_f1_score improved from 0.04665 to 0.08630, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 27s 647ms/step - loss: 0.7301 - accuracy: 0.9995 - recall: 0.9156 - precision: 0.5630 - f1_score: 0.7041 - IoU: 0.8104 - val_loss: 0.9189 - val_accuracy: 0.9994 - val_recall: 0.0675 - val_precision: 0.8809 - val_f1_score: 0.0863 - val_IoU: 0.5191\n",
      "Epoch 9/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4370 - accuracy: 0.9998 - recall: 0.9037 - precision: 0.7871 - f1_score: 0.8387 - IoU: 0.8609\n",
      "Epoch 9: val_f1_score improved from 0.08630 to 0.22738, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 31s 751ms/step - loss: 0.4370 - accuracy: 0.9998 - recall: 0.9037 - precision: 0.7871 - f1_score: 0.8387 - IoU: 0.8609 - val_loss: 0.8059 - val_accuracy: 0.9995 - val_recall: 0.1661 - val_precision: 0.9519 - val_f1_score: 0.2274 - val_IoU: 0.5339\n",
      "Epoch 10/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9999 - recall: 0.9266 - precision: 0.8916 - f1_score: 0.9063 - IoU: 0.8925\n",
      "Epoch 10: val_f1_score improved from 0.22738 to 0.37351, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 660ms/step - loss: 0.2227 - accuracy: 0.9999 - recall: 0.9266 - precision: 0.8916 - f1_score: 0.9063 - IoU: 0.8925 - val_loss: 0.6719 - val_accuracy: 0.9995 - val_recall: 0.2134 - val_precision: 0.9820 - val_f1_score: 0.3735 - val_IoU: 0.5497\n",
      "Epoch 11/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9998 - recall: 0.8816 - precision: 0.8630 - f1_score: 0.8638 - IoU: 0.8642\n",
      "Epoch 11: val_f1_score did not improve from 0.37351\n",
      "42/42 [==============================] - 29s 685ms/step - loss: 0.2016 - accuracy: 0.9998 - recall: 0.8816 - precision: 0.8630 - f1_score: 0.8638 - IoU: 0.8642 - val_loss: 0.9970 - val_accuracy: 0.9994 - val_recall: 0.0025 - val_precision: 1.0000 - val_f1_score: 0.0028 - val_IoU: 0.4998\n",
      "Epoch 12/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9999 - recall: 0.9263 - precision: 0.9039 - f1_score: 0.9113 - IoU: 0.8981\n",
      "Epoch 12: val_f1_score did not improve from 0.37351\n",
      "42/42 [==============================] - 27s 641ms/step - loss: 0.1344 - accuracy: 0.9999 - recall: 0.9263 - precision: 0.9039 - f1_score: 0.9113 - IoU: 0.8981 - val_loss: 0.6852 - val_accuracy: 0.9995 - val_recall: 0.2286 - val_precision: 0.9662 - val_f1_score: 0.3227 - val_IoU: 0.5627\n",
      "Epoch 13/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1018 - accuracy: 0.9999 - recall: 0.9304 - precision: 0.9370 - f1_score: 0.9312 - IoU: 0.9118\n",
      "Epoch 13: val_f1_score improved from 0.37351 to 0.53461, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 29s 686ms/step - loss: 0.1018 - accuracy: 0.9999 - recall: 0.9304 - precision: 0.9370 - f1_score: 0.9312 - IoU: 0.9118 - val_loss: 0.4761 - val_accuracy: 0.9990 - val_recall: 0.9798 - val_precision: 0.3758 - val_f1_score: 0.5346 - val_IoU: 0.7638\n",
      "Epoch 14/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9999 - recall: 0.9244 - precision: 0.9292 - f1_score: 0.9228 - IoU: 0.9046\n",
      "Epoch 14: val_f1_score improved from 0.53461 to 0.79113, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 663ms/step - loss: 0.1033 - accuracy: 0.9999 - recall: 0.9244 - precision: 0.9292 - f1_score: 0.9228 - IoU: 0.9046 - val_loss: 0.2358 - val_accuracy: 0.9998 - val_recall: 0.7542 - val_precision: 0.9913 - val_f1_score: 0.7911 - val_IoU: 0.7943\n",
      "Epoch 15/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9999 - recall: 0.9477 - precision: 0.9457 - f1_score: 0.9444 - IoU: 0.9263\n",
      "Epoch 15: val_f1_score improved from 0.79113 to 0.93938, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 678ms/step - loss: 0.0756 - accuracy: 0.9999 - recall: 0.9477 - precision: 0.9457 - f1_score: 0.9444 - IoU: 0.9263 - val_loss: 0.1113 - val_accuracy: 0.9999 - val_recall: 0.9278 - val_precision: 0.9831 - val_f1_score: 0.9394 - val_IoU: 0.9004\n",
      "Epoch 16/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9999 - recall: 0.9518 - precision: 0.9482 - f1_score: 0.9483 - IoU: 0.9300\n",
      "Epoch 16: val_f1_score did not improve from 0.93938\n",
      "42/42 [==============================] - 29s 689ms/step - loss: 0.0679 - accuracy: 0.9999 - recall: 0.9518 - precision: 0.9482 - f1_score: 0.9483 - IoU: 0.9300 - val_loss: 0.1537 - val_accuracy: 0.9999 - val_recall: 0.9117 - val_precision: 0.9898 - val_f1_score: 0.9354 - val_IoU: 0.8852\n",
      "Epoch 17/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9999 - recall: 0.9477 - precision: 0.9527 - f1_score: 0.9495 - IoU: 0.9309\n",
      "Epoch 17: val_f1_score improved from 0.93938 to 0.95191, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 31s 731ms/step - loss: 0.0641 - accuracy: 0.9999 - recall: 0.9477 - precision: 0.9527 - f1_score: 0.9495 - IoU: 0.9309 - val_loss: 0.1184 - val_accuracy: 0.9999 - val_recall: 0.9248 - val_precision: 0.9889 - val_f1_score: 0.9519 - val_IoU: 0.9035\n",
      "Epoch 18/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0573 - accuracy: 0.9999 - recall: 0.9526 - precision: 0.9580 - f1_score: 0.9549 - IoU: 0.9353\n",
      "Epoch 18: val_f1_score did not improve from 0.95191\n",
      "42/42 [==============================] - 29s 682ms/step - loss: 0.0573 - accuracy: 0.9999 - recall: 0.9526 - precision: 0.9580 - f1_score: 0.9549 - IoU: 0.9353 - val_loss: 0.1062 - val_accuracy: 0.9999 - val_recall: 0.9160 - val_precision: 0.9774 - val_f1_score: 0.9481 - val_IoU: 0.8954\n",
      "Epoch 19/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9999 - recall: 0.9562 - precision: 0.9528 - f1_score: 0.9528 - IoU: 0.9386\n",
      "Epoch 19: val_f1_score did not improve from 0.95191\n",
      "42/42 [==============================] - 28s 669ms/step - loss: 0.0577 - accuracy: 0.9999 - recall: 0.9562 - precision: 0.9528 - f1_score: 0.9528 - IoU: 0.9386 - val_loss: 0.1310 - val_accuracy: 0.9999 - val_recall: 0.8449 - val_precision: 0.9877 - val_f1_score: 0.9131 - val_IoU: 0.8620\n",
      "Epoch 20/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9999 - recall: 0.9526 - precision: 0.9539 - f1_score: 0.9520 - IoU: 0.9353\n",
      "Epoch 20: val_f1_score did not improve from 0.95191\n",
      "42/42 [==============================] - 29s 683ms/step - loss: 0.0570 - accuracy: 0.9999 - recall: 0.9526 - precision: 0.9539 - f1_score: 0.9520 - IoU: 0.9353 - val_loss: 0.1513 - val_accuracy: 0.9999 - val_recall: 0.7938 - val_precision: 0.9930 - val_f1_score: 0.8640 - val_IoU: 0.8437\n",
      "Epoch 21/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9999 - recall: 0.9600 - precision: 0.9587 - f1_score: 0.9554 - IoU: 0.9444\n",
      "Epoch 21: val_f1_score improved from 0.95191 to 0.96306, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 680ms/step - loss: 0.0523 - accuracy: 0.9999 - recall: 0.9600 - precision: 0.9587 - f1_score: 0.9554 - IoU: 0.9444 - val_loss: 0.0643 - val_accuracy: 1.0000 - val_recall: 0.9489 - val_precision: 0.9872 - val_f1_score: 0.9631 - val_IoU: 0.9308\n",
      "Epoch 22/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9999 - recall: 0.9605 - precision: 0.9575 - f1_score: 0.9580 - IoU: 0.9451\n",
      "Epoch 22: val_f1_score improved from 0.96306 to 0.96841, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 666ms/step - loss: 0.0490 - accuracy: 0.9999 - recall: 0.9605 - precision: 0.9575 - f1_score: 0.9580 - IoU: 0.9451 - val_loss: 0.0601 - val_accuracy: 1.0000 - val_recall: 0.9604 - val_precision: 0.9851 - val_f1_score: 0.9684 - val_IoU: 0.9451\n",
      "Epoch 23/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9999 - recall: 0.9510 - precision: 0.9506 - f1_score: 0.9499 - IoU: 0.9370\n",
      "Epoch 23: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 29s 699ms/step - loss: 0.0568 - accuracy: 0.9999 - recall: 0.9510 - precision: 0.9506 - f1_score: 0.9499 - IoU: 0.9370 - val_loss: 0.1099 - val_accuracy: 0.9999 - val_recall: 0.8503 - val_precision: 0.9854 - val_f1_score: 0.9099 - val_IoU: 0.8797\n",
      "Epoch 24/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9999 - recall: 0.9606 - precision: 0.9565 - f1_score: 0.9553 - IoU: 0.9450\n",
      "Epoch 24: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 29s 684ms/step - loss: 0.0507 - accuracy: 0.9999 - recall: 0.9606 - precision: 0.9565 - f1_score: 0.9553 - IoU: 0.9450 - val_loss: 0.0658 - val_accuracy: 1.0000 - val_recall: 0.9457 - val_precision: 0.9869 - val_f1_score: 0.9592 - val_IoU: 0.9414\n",
      "Epoch 25/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9999 - recall: 0.9596 - precision: 0.9592 - f1_score: 0.9574 - IoU: 0.9432\n",
      "Epoch 25: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 28s 681ms/step - loss: 0.0480 - accuracy: 0.9999 - recall: 0.9596 - precision: 0.9592 - f1_score: 0.9574 - IoU: 0.9432 - val_loss: 0.0640 - val_accuracy: 1.0000 - val_recall: 0.9418 - val_precision: 0.9888 - val_f1_score: 0.9625 - val_IoU: 0.9372\n",
      "Epoch 26/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9999 - recall: 0.9572 - precision: 0.9582 - f1_score: 0.9571 - IoU: 0.9423\n",
      "Epoch 26: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 27s 636ms/step - loss: 0.0478 - accuracy: 0.9999 - recall: 0.9572 - precision: 0.9582 - f1_score: 0.9571 - IoU: 0.9423 - val_loss: 0.2126 - val_accuracy: 0.9998 - val_recall: 0.6140 - val_precision: 0.9929 - val_f1_score: 0.8074 - val_IoU: 0.7553\n",
      "Epoch 27/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9999 - recall: 0.8886 - precision: 0.9012 - f1_score: 0.8937 - IoU: 0.8952\n",
      "Epoch 27: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 26s 621ms/step - loss: 0.1117 - accuracy: 0.9999 - recall: 0.8886 - precision: 0.9012 - f1_score: 0.8937 - IoU: 0.8952 - val_loss: 0.3242 - val_accuracy: 0.9997 - val_recall: 0.5357 - val_precision: 0.9165 - val_f1_score: 0.6905 - val_IoU: 0.7421\n",
      "Epoch 28/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9999 - recall: 0.9420 - precision: 0.9385 - f1_score: 0.9384 - IoU: 0.9307\n",
      "Epoch 28: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 26s 626ms/step - loss: 0.0659 - accuracy: 0.9999 - recall: 0.9420 - precision: 0.9385 - f1_score: 0.9384 - IoU: 0.9307 - val_loss: 0.1291 - val_accuracy: 0.9999 - val_recall: 0.9711 - val_precision: 0.8566 - val_f1_score: 0.8801 - val_IoU: 0.9168\n",
      "Epoch 29/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9999 - recall: 0.9287 - precision: 0.9249 - f1_score: 0.9252 - IoU: 0.9154\n",
      "Epoch 29: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 25s 607ms/step - loss: 0.0786 - accuracy: 0.9999 - recall: 0.9287 - precision: 0.9249 - f1_score: 0.9252 - IoU: 0.9154 - val_loss: 0.1090 - val_accuracy: 0.9999 - val_recall: 0.9489 - val_precision: 0.9006 - val_f1_score: 0.9034 - val_IoU: 0.9047\n",
      "Epoch 30/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9999 - recall: 0.9390 - precision: 0.9348 - f1_score: 0.9353 - IoU: 0.9285\n",
      "Epoch 30: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 26s 613ms/step - loss: 0.0686 - accuracy: 0.9999 - recall: 0.9390 - precision: 0.9348 - f1_score: 0.9353 - IoU: 0.9285 - val_loss: 0.1112 - val_accuracy: 0.9999 - val_recall: 0.8531 - val_precision: 0.9917 - val_f1_score: 0.8997 - val_IoU: 0.8856\n",
      "Epoch 31/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9999 - recall: 0.9505 - precision: 0.9428 - f1_score: 0.9441 - IoU: 0.9357\n",
      "Epoch 31: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 26s 618ms/step - loss: 0.0594 - accuracy: 0.9999 - recall: 0.9505 - precision: 0.9428 - f1_score: 0.9441 - IoU: 0.9357 - val_loss: 0.0476 - val_accuracy: 1.0000 - val_recall: 0.9400 - val_precision: 0.9891 - val_f1_score: 0.9624 - val_IoU: 0.9364\n",
      "Epoch 32/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9999 - recall: 0.9578 - precision: 0.9563 - f1_score: 0.9549 - IoU: 0.9444\n",
      "Epoch 32: val_f1_score did not improve from 0.96841\n",
      "42/42 [==============================] - 26s 624ms/step - loss: 0.0483 - accuracy: 0.9999 - recall: 0.9578 - precision: 0.9563 - f1_score: 0.9549 - IoU: 0.9444 - val_loss: 0.0423 - val_accuracy: 1.0000 - val_recall: 0.9575 - val_precision: 0.9848 - val_f1_score: 0.9677 - val_IoU: 0.9492\n",
      "Epoch 33/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 1.0000 - recall: 0.9616 - precision: 0.9602 - f1_score: 0.9599 - IoU: 0.9488\n",
      "Epoch 33: val_f1_score improved from 0.96841 to 0.97107, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 27s 654ms/step - loss: 0.0431 - accuracy: 1.0000 - recall: 0.9616 - precision: 0.9602 - f1_score: 0.9599 - IoU: 0.9488 - val_loss: 0.0401 - val_accuracy: 1.0000 - val_recall: 0.9777 - val_precision: 0.9705 - val_f1_score: 0.9711 - val_IoU: 0.9626\n",
      "Epoch 34/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 1.0000 - recall: 0.9647 - precision: 0.9604 - f1_score: 0.9627 - IoU: 0.9507\n",
      "Epoch 34: val_f1_score did not improve from 0.97107\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 0.0402 - accuracy: 1.0000 - recall: 0.9647 - precision: 0.9604 - f1_score: 0.9627 - IoU: 0.9507 - val_loss: 0.0434 - val_accuracy: 1.0000 - val_recall: 0.9540 - val_precision: 0.9875 - val_f1_score: 0.9688 - val_IoU: 0.9498\n",
      "Epoch 35/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 1.0000 - recall: 0.9603 - precision: 0.9628 - f1_score: 0.9610 - IoU: 0.9492\n",
      "Epoch 35: val_f1_score improved from 0.97107 to 0.97337, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 26s 631ms/step - loss: 0.0417 - accuracy: 1.0000 - recall: 0.9603 - precision: 0.9628 - f1_score: 0.9610 - IoU: 0.9492 - val_loss: 0.0325 - val_accuracy: 1.0000 - val_recall: 0.9763 - val_precision: 0.9753 - val_f1_score: 0.9734 - val_IoU: 0.9617\n",
      "Epoch 36/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 1.0000 - recall: 0.9651 - precision: 0.9608 - f1_score: 0.9618 - IoU: 0.9533\n",
      "Epoch 36: val_f1_score did not improve from 0.97337\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 0.0405 - accuracy: 1.0000 - recall: 0.9651 - precision: 0.9608 - f1_score: 0.9618 - IoU: 0.9533 - val_loss: 0.0381 - val_accuracy: 1.0000 - val_recall: 0.9598 - val_precision: 0.9858 - val_f1_score: 0.9702 - val_IoU: 0.9534\n",
      "Epoch 37/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 1.0000 - recall: 0.9654 - precision: 0.9619 - f1_score: 0.9639 - IoU: 0.9539\n",
      "Epoch 37: val_f1_score did not improve from 0.97337\n",
      "42/42 [==============================] - 28s 667ms/step - loss: 0.0383 - accuracy: 1.0000 - recall: 0.9654 - precision: 0.9619 - f1_score: 0.9639 - IoU: 0.9539 - val_loss: 0.0392 - val_accuracy: 1.0000 - val_recall: 0.9896 - val_precision: 0.9520 - val_f1_score: 0.9686 - val_IoU: 0.9751\n",
      "Epoch 38/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9651 - precision: 0.9622 - f1_score: 0.9632 - IoU: 0.9539\n",
      "Epoch 38: val_f1_score improved from 0.97337 to 0.97345, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 29s 701ms/step - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9651 - precision: 0.9622 - f1_score: 0.9632 - IoU: 0.9539 - val_loss: 0.0332 - val_accuracy: 1.0000 - val_recall: 0.9821 - val_precision: 0.9713 - val_f1_score: 0.9735 - val_IoU: 0.9704\n",
      "Epoch 39/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9644 - precision: 0.9619 - f1_score: 0.9630 - IoU: 0.9537\n",
      "Epoch 39: val_f1_score improved from 0.97345 to 0.97472, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 677ms/step - loss: 0.0390 - accuracy: 1.0000 - recall: 0.9644 - precision: 0.9619 - f1_score: 0.9630 - IoU: 0.9537 - val_loss: 0.0337 - val_accuracy: 1.0000 - val_recall: 0.9741 - val_precision: 0.9795 - val_f1_score: 0.9747 - val_IoU: 0.9638\n",
      "Epoch 40/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 1.0000 - recall: 0.9684 - precision: 0.9625 - f1_score: 0.9640 - IoU: 0.9560\n",
      "Epoch 40: val_f1_score improved from 0.97472 to 0.97539, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 675ms/step - loss: 0.0381 - accuracy: 1.0000 - recall: 0.9684 - precision: 0.9625 - f1_score: 0.9640 - IoU: 0.9560 - val_loss: 0.0339 - val_accuracy: 1.0000 - val_recall: 0.9707 - val_precision: 0.9832 - val_f1_score: 0.9754 - val_IoU: 0.9626\n",
      "Epoch 41/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000 - recall: 0.9620 - precision: 0.9665 - f1_score: 0.9638 - IoU: 0.9534\n",
      "Epoch 41: val_f1_score did not improve from 0.97539\n",
      "42/42 [==============================] - 27s 645ms/step - loss: 0.0380 - accuracy: 1.0000 - recall: 0.9620 - precision: 0.9665 - f1_score: 0.9638 - IoU: 0.9534 - val_loss: 0.0351 - val_accuracy: 1.0000 - val_recall: 0.9615 - val_precision: 0.9867 - val_f1_score: 0.9725 - val_IoU: 0.9561\n",
      "Epoch 42/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 1.0000 - recall: 0.9633 - precision: 0.9653 - f1_score: 0.9642 - IoU: 0.9548\n",
      "Epoch 42: val_f1_score did not improve from 0.97539\n",
      "42/42 [==============================] - 27s 653ms/step - loss: 0.0376 - accuracy: 1.0000 - recall: 0.9633 - precision: 0.9653 - f1_score: 0.9642 - IoU: 0.9548 - val_loss: 0.0311 - val_accuracy: 1.0000 - val_recall: 0.9718 - val_precision: 0.9751 - val_f1_score: 0.9722 - val_IoU: 0.9628\n",
      "Epoch 43/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 1.0000 - recall: 0.9680 - precision: 0.9643 - f1_score: 0.9657 - IoU: 0.9570\n",
      "Epoch 43: val_f1_score improved from 0.97539 to 0.97553, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 29s 686ms/step - loss: 0.0360 - accuracy: 1.0000 - recall: 0.9680 - precision: 0.9643 - f1_score: 0.9657 - IoU: 0.9570 - val_loss: 0.0273 - val_accuracy: 1.0000 - val_recall: 0.9768 - val_precision: 0.9784 - val_f1_score: 0.9755 - val_IoU: 0.9658\n",
      "Epoch 44/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9650 - f1_score: 0.9652 - IoU: 0.9553\n",
      "Epoch 44: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 28s 658ms/step - loss: 0.0363 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9650 - f1_score: 0.9652 - IoU: 0.9553 - val_loss: 0.0294 - val_accuracy: 1.0000 - val_recall: 0.9672 - val_precision: 0.9853 - val_f1_score: 0.9749 - val_IoU: 0.9585\n",
      "Epoch 45/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 1.0000 - recall: 0.9628 - precision: 0.9650 - f1_score: 0.9634 - IoU: 0.9531\n",
      "Epoch 45: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 29s 685ms/step - loss: 0.0382 - accuracy: 1.0000 - recall: 0.9628 - precision: 0.9650 - f1_score: 0.9634 - IoU: 0.9531 - val_loss: 0.0330 - val_accuracy: 1.0000 - val_recall: 0.9696 - val_precision: 0.9785 - val_f1_score: 0.9692 - val_IoU: 0.9622\n",
      "Epoch 46/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000 - recall: 0.9663 - precision: 0.9658 - f1_score: 0.9660 - IoU: 0.9574\n",
      "Epoch 46: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 28s 662ms/step - loss: 0.0354 - accuracy: 1.0000 - recall: 0.9663 - precision: 0.9658 - f1_score: 0.9660 - IoU: 0.9574 - val_loss: 0.0295 - val_accuracy: 1.0000 - val_recall: 0.9646 - val_precision: 0.9842 - val_f1_score: 0.9721 - val_IoU: 0.9589\n",
      "Epoch 47/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9999 - recall: 0.9458 - precision: 0.9539 - f1_score: 0.9463 - IoU: 0.9420\n",
      "Epoch 47: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 28s 670ms/step - loss: 0.0549 - accuracy: 0.9999 - recall: 0.9458 - precision: 0.9539 - f1_score: 0.9463 - IoU: 0.9420 - val_loss: 0.2139 - val_accuracy: 0.9998 - val_recall: 0.8480 - val_precision: 0.8942 - val_f1_score: 0.7790 - val_IoU: 0.8780\n",
      "Epoch 48/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9999 - recall: 0.9181 - precision: 0.9370 - f1_score: 0.9207 - IoU: 0.9215\n",
      "Epoch 48: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 27s 642ms/step - loss: 0.0807 - accuracy: 0.9999 - recall: 0.9181 - precision: 0.9370 - f1_score: 0.9207 - IoU: 0.9215 - val_loss: 0.5100 - val_accuracy: 0.9996 - val_recall: 0.3658 - val_precision: 0.9892 - val_f1_score: 0.4956 - val_IoU: 0.6619\n",
      "Epoch 49/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9999 - recall: 0.9496 - precision: 0.9536 - f1_score: 0.9505 - IoU: 0.9438\n",
      "Epoch 49: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 31s 751ms/step - loss: 0.0509 - accuracy: 0.9999 - recall: 0.9496 - precision: 0.9536 - f1_score: 0.9505 - IoU: 0.9438 - val_loss: 0.0747 - val_accuracy: 0.9999 - val_recall: 0.9107 - val_precision: 0.9769 - val_f1_score: 0.9313 - val_IoU: 0.9227\n",
      "Epoch 50/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9999 - recall: 0.9349 - precision: 0.9429 - f1_score: 0.9361 - IoU: 0.9313\n",
      "Epoch 50: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 27s 653ms/step - loss: 0.0653 - accuracy: 0.9999 - recall: 0.9349 - precision: 0.9429 - f1_score: 0.9361 - IoU: 0.9313 - val_loss: 0.0763 - val_accuracy: 0.9999 - val_recall: 0.8938 - val_precision: 0.9835 - val_f1_score: 0.9283 - val_IoU: 0.9145\n",
      "Epoch 51/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9999 - recall: 0.9504 - precision: 0.9496 - f1_score: 0.9482 - IoU: 0.9418\n",
      "Epoch 51: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 28s 666ms/step - loss: 0.0532 - accuracy: 0.9999 - recall: 0.9504 - precision: 0.9496 - f1_score: 0.9482 - IoU: 0.9418 - val_loss: 0.0703 - val_accuracy: 0.9999 - val_recall: 0.8800 - val_precision: 0.9896 - val_f1_score: 0.9335 - val_IoU: 0.9083\n",
      "Epoch 52/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 1.0000 - recall: 0.9633 - precision: 0.9578 - f1_score: 0.9587 - IoU: 0.9529\n",
      "Epoch 52: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 27s 640ms/step - loss: 0.0426 - accuracy: 1.0000 - recall: 0.9633 - precision: 0.9578 - f1_score: 0.9587 - IoU: 0.9529 - val_loss: 0.0402 - val_accuracy: 1.0000 - val_recall: 0.9357 - val_precision: 0.9864 - val_f1_score: 0.9629 - val_IoU: 0.9402\n",
      "Epoch 53/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9999 - recall: 0.9433 - precision: 0.9345 - f1_score: 0.9376 - IoU: 0.9314\n",
      "Epoch 53: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 26s 630ms/step - loss: 0.0636 - accuracy: 0.9999 - recall: 0.9433 - precision: 0.9345 - f1_score: 0.9376 - IoU: 0.9314 - val_loss: 0.0484 - val_accuracy: 0.9999 - val_recall: 0.9154 - val_precision: 0.9888 - val_f1_score: 0.9542 - val_IoU: 0.9253\n",
      "Epoch 54/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 1.0000 - recall: 0.9606 - precision: 0.9615 - f1_score: 0.9602 - IoU: 0.9509\n",
      "Epoch 54: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 27s 637ms/step - loss: 0.0411 - accuracy: 1.0000 - recall: 0.9606 - precision: 0.9615 - f1_score: 0.9602 - IoU: 0.9509 - val_loss: 0.0300 - val_accuracy: 1.0000 - val_recall: 0.9784 - val_precision: 0.9712 - val_f1_score: 0.9719 - val_IoU: 0.9679\n",
      "Epoch 55/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 1.0000 - recall: 0.9673 - precision: 0.9602 - f1_score: 0.9622 - IoU: 0.9562\n",
      "Epoch 55: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 29s 686ms/step - loss: 0.0389 - accuracy: 1.0000 - recall: 0.9673 - precision: 0.9602 - f1_score: 0.9622 - IoU: 0.9562 - val_loss: 0.0323 - val_accuracy: 1.0000 - val_recall: 0.9642 - val_precision: 0.9837 - val_f1_score: 0.9697 - val_IoU: 0.9591\n",
      "Epoch 56/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9659 - precision: 0.9643 - f1_score: 0.9645 - IoU: 0.9566\n",
      "Epoch 56: val_f1_score did not improve from 0.97553\n",
      "42/42 [==============================] - 26s 626ms/step - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9659 - precision: 0.9643 - f1_score: 0.9645 - IoU: 0.9566 - val_loss: 0.0333 - val_accuracy: 1.0000 - val_recall: 0.9602 - val_precision: 0.9821 - val_f1_score: 0.9683 - val_IoU: 0.9558\n",
      "Epoch 57/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 1.0000 - recall: 0.9678 - precision: 0.9653 - f1_score: 0.9655 - IoU: 0.9570\n",
      "Epoch 57: val_f1_score improved from 0.97553 to 0.97566, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 29s 685ms/step - loss: 0.0355 - accuracy: 1.0000 - recall: 0.9678 - precision: 0.9653 - f1_score: 0.9655 - IoU: 0.9570 - val_loss: 0.0258 - val_accuracy: 1.0000 - val_recall: 0.9770 - val_precision: 0.9772 - val_f1_score: 0.9757 - val_IoU: 0.9681\n",
      "Epoch 58/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 1.0000 - recall: 0.9687 - precision: 0.9649 - f1_score: 0.9668 - IoU: 0.9568\n",
      "Epoch 58: val_f1_score improved from 0.97566 to 0.97632, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 36s 856ms/step - loss: 0.0342 - accuracy: 1.0000 - recall: 0.9687 - precision: 0.9649 - f1_score: 0.9668 - IoU: 0.9568 - val_loss: 0.0242 - val_accuracy: 1.0000 - val_recall: 0.9718 - val_precision: 0.9837 - val_f1_score: 0.9763 - val_IoU: 0.9643\n",
      "Epoch 59/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9664 - f1_score: 0.9674 - IoU: 0.9584\n",
      "Epoch 59: val_f1_score improved from 0.97632 to 0.97689, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 28s 680ms/step - loss: 0.0334 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9664 - f1_score: 0.9674 - IoU: 0.9584 - val_loss: 0.0236 - val_accuracy: 1.0000 - val_recall: 0.9771 - val_precision: 0.9803 - val_f1_score: 0.9769 - val_IoU: 0.9683\n",
      "Epoch 60/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000 - recall: 0.9686 - precision: 0.9664 - f1_score: 0.9656 - IoU: 0.9582\n",
      "Epoch 60: val_f1_score improved from 0.97689 to 0.97711, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 32s 767ms/step - loss: 0.0353 - accuracy: 1.0000 - recall: 0.9686 - precision: 0.9664 - f1_score: 0.9656 - IoU: 0.9582 - val_loss: 0.0237 - val_accuracy: 1.0000 - val_recall: 0.9755 - val_precision: 0.9820 - val_f1_score: 0.9771 - val_IoU: 0.9669\n",
      "Epoch 61/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 1.0000 - recall: 0.9699 - precision: 0.9661 - f1_score: 0.9668 - IoU: 0.9582\n",
      "Epoch 61: val_f1_score did not improve from 0.97711\n",
      "42/42 [==============================] - 26s 627ms/step - loss: 0.0341 - accuracy: 1.0000 - recall: 0.9699 - precision: 0.9661 - f1_score: 0.9668 - IoU: 0.9582 - val_loss: 0.0241 - val_accuracy: 1.0000 - val_recall: 0.9708 - val_precision: 0.9843 - val_f1_score: 0.9768 - val_IoU: 0.9642\n",
      "Epoch 62/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9688 - precision: 0.9678 - f1_score: 0.9674 - IoU: 0.9588\n",
      "Epoch 62: val_f1_score did not improve from 0.97711\n",
      "42/42 [==============================] - 31s 745ms/step - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9688 - precision: 0.9678 - f1_score: 0.9674 - IoU: 0.9588 - val_loss: 0.0363 - val_accuracy: 1.0000 - val_recall: 0.9832 - val_precision: 0.9620 - val_f1_score: 0.9651 - val_IoU: 0.9692\n",
      "Epoch 63/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9681 - precision: 0.9676 - f1_score: 0.9667 - IoU: 0.9596\n",
      "Epoch 63: val_f1_score did not improve from 0.97711\n",
      "42/42 [==============================] - 30s 723ms/step - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9681 - precision: 0.9676 - f1_score: 0.9667 - IoU: 0.9596 - val_loss: 0.0303 - val_accuracy: 1.0000 - val_recall: 0.9535 - val_precision: 0.9904 - val_f1_score: 0.9708 - val_IoU: 0.9543\n",
      "Epoch 64/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9672 - f1_score: 0.9641 - IoU: 0.9582\n",
      "Epoch 64: val_f1_score did not improve from 0.97711\n",
      "42/42 [==============================] - 26s 631ms/step - loss: 0.0366 - accuracy: 1.0000 - recall: 0.9662 - precision: 0.9672 - f1_score: 0.9641 - IoU: 0.9582 - val_loss: 0.0314 - val_accuracy: 1.0000 - val_recall: 0.9531 - val_precision: 0.9870 - val_f1_score: 0.9695 - val_IoU: 0.9527\n",
      "Epoch 65/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9703 - precision: 0.9662 - f1_score: 0.9672 - IoU: 0.9612\n",
      "Epoch 65: val_f1_score did not improve from 0.97711\n",
      "42/42 [==============================] - 29s 695ms/step - loss: 0.0335 - accuracy: 1.0000 - recall: 0.9703 - precision: 0.9662 - f1_score: 0.9672 - IoU: 0.9612 - val_loss: 0.0239 - val_accuracy: 1.0000 - val_recall: 0.9744 - val_precision: 0.9822 - val_f1_score: 0.9768 - val_IoU: 0.9680\n",
      "Epoch 66/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9701 - precision: 0.9664 - f1_score: 0.9680 - IoU: 0.9611\n",
      "Epoch 66: val_f1_score improved from 0.97711 to 0.97773, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 29s 693ms/step - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9701 - precision: 0.9664 - f1_score: 0.9680 - IoU: 0.9611 - val_loss: 0.0236 - val_accuracy: 1.0000 - val_recall: 0.9818 - val_precision: 0.9778 - val_f1_score: 0.9777 - val_IoU: 0.9735\n",
      "Epoch 67/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9671 - f1_score: 0.9682 - IoU: 0.9621\n",
      "Epoch 67: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 31s 752ms/step - loss: 0.0325 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9671 - f1_score: 0.9682 - IoU: 0.9621 - val_loss: 0.0277 - val_accuracy: 1.0000 - val_recall: 0.9581 - val_precision: 0.9912 - val_f1_score: 0.9738 - val_IoU: 0.9584\n",
      "Epoch 68/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 1.0000 - recall: 0.9701 - precision: 0.9675 - f1_score: 0.9675 - IoU: 0.9610\n",
      "Epoch 68: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 58s 1s/step - loss: 0.0331 - accuracy: 1.0000 - recall: 0.9701 - precision: 0.9675 - f1_score: 0.9675 - IoU: 0.9610 - val_loss: 0.0276 - val_accuracy: 1.0000 - val_recall: 0.9579 - val_precision: 0.9910 - val_f1_score: 0.9726 - val_IoU: 0.9566\n",
      "Epoch 69/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9665 - precision: 0.9682 - f1_score: 0.9668 - IoU: 0.9581\n",
      "Epoch 69: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 34s 820ms/step - loss: 0.0340 - accuracy: 1.0000 - recall: 0.9665 - precision: 0.9682 - f1_score: 0.9668 - IoU: 0.9581 - val_loss: 0.0247 - val_accuracy: 1.0000 - val_recall: 0.9852 - val_precision: 0.9728 - val_f1_score: 0.9754 - val_IoU: 0.9731\n",
      "Epoch 70/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9697 - precision: 0.9675 - f1_score: 0.9670 - IoU: 0.9600\n",
      "Epoch 70: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 31s 734ms/step - loss: 0.0336 - accuracy: 1.0000 - recall: 0.9697 - precision: 0.9675 - f1_score: 0.9670 - IoU: 0.9600 - val_loss: 0.0246 - val_accuracy: 1.0000 - val_recall: 0.9788 - val_precision: 0.9783 - val_f1_score: 0.9758 - val_IoU: 0.9708\n",
      "Epoch 71/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9708 - precision: 0.9698 - f1_score: 0.9704 - IoU: 0.9615\n",
      "Epoch 71: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 32s 773ms/step - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9708 - precision: 0.9698 - f1_score: 0.9704 - IoU: 0.9615 - val_loss: 0.0256 - val_accuracy: 1.0000 - val_recall: 0.9688 - val_precision: 0.9868 - val_f1_score: 0.9756 - val_IoU: 0.9636\n",
      "Epoch 72/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9675 - f1_score: 0.9686 - IoU: 0.9621\n",
      "Epoch 72: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 35s 829ms/step - loss: 0.0321 - accuracy: 1.0000 - recall: 0.9709 - precision: 0.9675 - f1_score: 0.9686 - IoU: 0.9621 - val_loss: 0.0261 - val_accuracy: 1.0000 - val_recall: 0.9667 - val_precision: 0.9863 - val_f1_score: 0.9752 - val_IoU: 0.9642\n",
      "Epoch 73/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000 - recall: 0.9664 - precision: 0.9706 - f1_score: 0.9676 - IoU: 0.9596\n",
      "Epoch 73: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 32s 747ms/step - loss: 0.0328 - accuracy: 1.0000 - recall: 0.9664 - precision: 0.9706 - f1_score: 0.9676 - IoU: 0.9596 - val_loss: 0.0252 - val_accuracy: 1.0000 - val_recall: 0.9648 - val_precision: 0.9886 - val_f1_score: 0.9753 - val_IoU: 0.9615\n",
      "Epoch 74/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9720 - precision: 0.9688 - f1_score: 0.9697 - IoU: 0.9625\n",
      "Epoch 74: val_f1_score did not improve from 0.97773\n",
      "42/42 [==============================] - 34s 812ms/step - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9720 - precision: 0.9688 - f1_score: 0.9697 - IoU: 0.9625 - val_loss: 0.0262 - val_accuracy: 1.0000 - val_recall: 0.9625 - val_precision: 0.9906 - val_f1_score: 0.9746 - val_IoU: 0.9622\n",
      "Epoch 75/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9690 - f1_score: 0.9713 - IoU: 0.9639\n",
      "Epoch 75: val_f1_score improved from 0.97773 to 0.97799, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 33s 777ms/step - loss: 0.0294 - accuracy: 1.0000 - recall: 0.9740 - precision: 0.9690 - f1_score: 0.9713 - IoU: 0.9639 - val_loss: 0.0237 - val_accuracy: 1.0000 - val_recall: 0.9792 - val_precision: 0.9813 - val_f1_score: 0.9780 - val_IoU: 0.9723\n",
      "Epoch 76/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9684 - f1_score: 0.9701 - IoU: 0.9629\n",
      "Epoch 76: val_f1_score did not improve from 0.97799\n",
      "42/42 [==============================] - 30s 724ms/step - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9684 - f1_score: 0.9701 - IoU: 0.9629 - val_loss: 0.0270 - val_accuracy: 1.0000 - val_recall: 0.9608 - val_precision: 0.9905 - val_f1_score: 0.9739 - val_IoU: 0.9606\n",
      "Epoch 77/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9704 - f1_score: 0.9692 - IoU: 0.9622\n",
      "Epoch 77: val_f1_score improved from 0.97799 to 0.97871, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 77s 2s/step - loss: 0.0314 - accuracy: 1.0000 - recall: 0.9695 - precision: 0.9704 - f1_score: 0.9692 - IoU: 0.9622 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9744 - val_precision: 0.9859 - val_f1_score: 0.9787 - val_IoU: 0.9682\n",
      "Epoch 78/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9684 - f1_score: 0.9709 - IoU: 0.9637\n",
      "Epoch 78: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 29s 700ms/step - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9684 - f1_score: 0.9709 - IoU: 0.9637 - val_loss: 0.0224 - val_accuracy: 1.0000 - val_recall: 0.9731 - val_precision: 0.9877 - val_f1_score: 0.9780 - val_IoU: 0.9683\n",
      "Epoch 79/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9679 - precision: 0.9698 - f1_score: 0.9689 - IoU: 0.9601\n",
      "Epoch 79: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 129s 3s/step - loss: 0.0318 - accuracy: 1.0000 - recall: 0.9679 - precision: 0.9698 - f1_score: 0.9689 - IoU: 0.9601 - val_loss: 0.0296 - val_accuracy: 1.0000 - val_recall: 0.9524 - val_precision: 0.9904 - val_f1_score: 0.9709 - val_IoU: 0.9545\n",
      "Epoch 80/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9741 - precision: 0.9679 - f1_score: 0.9706 - IoU: 0.9639\n",
      "Epoch 80: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 32s 755ms/step - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9741 - precision: 0.9679 - f1_score: 0.9706 - IoU: 0.9639 - val_loss: 0.0284 - val_accuracy: 1.0000 - val_recall: 0.9549 - val_precision: 0.9916 - val_f1_score: 0.9724 - val_IoU: 0.9573\n",
      "Epoch 81/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9678 - f1_score: 0.9698 - IoU: 0.9638\n",
      "Epoch 81: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 30s 725ms/step - loss: 0.0308 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9678 - f1_score: 0.9698 - IoU: 0.9638 - val_loss: 0.0253 - val_accuracy: 1.0000 - val_recall: 0.9627 - val_precision: 0.9903 - val_f1_score: 0.9747 - val_IoU: 0.9617\n",
      "Epoch 82/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9684 - f1_score: 0.9695 - IoU: 0.9635\n",
      "Epoch 82: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 90s 2s/step - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9724 - precision: 0.9684 - f1_score: 0.9695 - IoU: 0.9635 - val_loss: 0.0296 - val_accuracy: 1.0000 - val_recall: 0.9597 - val_precision: 0.9859 - val_f1_score: 0.9716 - val_IoU: 0.9589\n",
      "Epoch 83/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9687 - f1_score: 0.9694 - IoU: 0.9640\n",
      "Epoch 83: val_f1_score did not improve from 0.97871\n",
      "42/42 [==============================] - 32s 747ms/step - loss: 0.0311 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9687 - f1_score: 0.9694 - IoU: 0.9640 - val_loss: 0.0221 - val_accuracy: 1.0000 - val_recall: 0.9900 - val_precision: 0.9695 - val_f1_score: 0.9782 - val_IoU: 0.9794\n",
      "Epoch 84/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9694 - f1_score: 0.9706 - IoU: 0.9634\n",
      "Epoch 84: val_f1_score improved from 0.97871 to 0.98002, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 38s 916ms/step - loss: 0.0299 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9694 - f1_score: 0.9706 - IoU: 0.9634 - val_loss: 0.0204 - val_accuracy: 1.0000 - val_recall: 0.9791 - val_precision: 0.9844 - val_f1_score: 0.9800 - val_IoU: 0.9724\n",
      "Epoch 85/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9760 - precision: 0.9684 - f1_score: 0.9716 - IoU: 0.9661\n",
      "Epoch 85: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 36s 845ms/step - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9760 - precision: 0.9684 - f1_score: 0.9716 - IoU: 0.9661 - val_loss: 0.0232 - val_accuracy: 1.0000 - val_recall: 0.9744 - val_precision: 0.9849 - val_f1_score: 0.9776 - val_IoU: 0.9691\n",
      "Epoch 86/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000 - recall: 0.9725 - precision: 0.9699 - f1_score: 0.9712 - IoU: 0.9642\n",
      "Epoch 86: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 30s 725ms/step - loss: 0.0292 - accuracy: 1.0000 - recall: 0.9725 - precision: 0.9699 - f1_score: 0.9712 - IoU: 0.9642 - val_loss: 0.0231 - val_accuracy: 1.0000 - val_recall: 0.9720 - val_precision: 0.9882 - val_f1_score: 0.9794 - val_IoU: 0.9677\n",
      "Epoch 87/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9699 - f1_score: 0.9728 - IoU: 0.9657\n",
      "Epoch 87: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 35s 829ms/step - loss: 0.0278 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9699 - f1_score: 0.9728 - IoU: 0.9657 - val_loss: 0.0214 - val_accuracy: 1.0000 - val_recall: 0.9730 - val_precision: 0.9886 - val_f1_score: 0.9795 - val_IoU: 0.9683\n",
      "Epoch 88/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000 - recall: 0.9716 - precision: 0.9698 - f1_score: 0.9708 - IoU: 0.9626\n",
      "Epoch 88: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 40s 953ms/step - loss: 0.0297 - accuracy: 1.0000 - recall: 0.9716 - precision: 0.9698 - f1_score: 0.9708 - IoU: 0.9626 - val_loss: 0.0212 - val_accuracy: 1.0000 - val_recall: 0.9740 - val_precision: 0.9871 - val_f1_score: 0.9790 - val_IoU: 0.9686\n",
      "Epoch 89/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9699 - f1_score: 0.9707 - IoU: 0.9647\n",
      "Epoch 89: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 31s 738ms/step - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9699 - f1_score: 0.9707 - IoU: 0.9647 - val_loss: 0.0256 - val_accuracy: 1.0000 - val_recall: 0.9613 - val_precision: 0.9921 - val_f1_score: 0.9750 - val_IoU: 0.9615\n",
      "Epoch 90/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000 - recall: 0.9719 - precision: 0.9710 - f1_score: 0.9703 - IoU: 0.9648\n",
      "Epoch 90: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 32s 767ms/step - loss: 0.0301 - accuracy: 1.0000 - recall: 0.9719 - precision: 0.9710 - f1_score: 0.9703 - IoU: 0.9648 - val_loss: 0.0205 - val_accuracy: 1.0000 - val_recall: 0.9888 - val_precision: 0.9754 - val_f1_score: 0.9797 - val_IoU: 0.9779\n",
      "Epoch 91/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9732 - precision: 0.9681 - f1_score: 0.9699 - IoU: 0.9638\n",
      "Epoch 91: val_f1_score did not improve from 0.98002\n",
      "42/42 [==============================] - 36s 846ms/step - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9732 - precision: 0.9681 - f1_score: 0.9699 - IoU: 0.9638 - val_loss: 0.0243 - val_accuracy: 1.0000 - val_recall: 0.9789 - val_precision: 0.9798 - val_f1_score: 0.9766 - val_IoU: 0.9735\n",
      "Epoch 92/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9696 - f1_score: 0.9697 - IoU: 0.9643\n",
      "Epoch 92: val_f1_score improved from 0.98002 to 0.98015, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 51s 1s/step - loss: 0.0307 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9696 - f1_score: 0.9697 - IoU: 0.9643 - val_loss: 0.0203 - val_accuracy: 1.0000 - val_recall: 0.9774 - val_precision: 0.9850 - val_f1_score: 0.9802 - val_IoU: 0.9718\n",
      "Epoch 93/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9736 - precision: 0.9699 - f1_score: 0.9716 - IoU: 0.9653\n",
      "Epoch 93: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 31s 730ms/step - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9736 - precision: 0.9699 - f1_score: 0.9716 - IoU: 0.9653 - val_loss: 0.0228 - val_accuracy: 1.0000 - val_recall: 0.9892 - val_precision: 0.9716 - val_f1_score: 0.9780 - val_IoU: 0.9792\n",
      "Epoch 94/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9688 - f1_score: 0.9715 - IoU: 0.9644\n",
      "Epoch 94: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 26s 620ms/step - loss: 0.0288 - accuracy: 1.0000 - recall: 0.9734 - precision: 0.9688 - f1_score: 0.9715 - IoU: 0.9644 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9862 - val_precision: 0.9773 - val_f1_score: 0.9791 - val_IoU: 0.9780\n",
      "Epoch 95/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9695 - f1_score: 0.9715 - IoU: 0.9655\n",
      "Epoch 95: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 32s 762ms/step - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9749 - precision: 0.9695 - f1_score: 0.9715 - IoU: 0.9655 - val_loss: 0.0233 - val_accuracy: 1.0000 - val_recall: 0.9858 - val_precision: 0.9767 - val_f1_score: 0.9768 - val_IoU: 0.9773\n",
      "Epoch 96/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9702 - f1_score: 0.9715 - IoU: 0.9663\n",
      "Epoch 96: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 74s 2s/step - loss: 0.0290 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9702 - f1_score: 0.9715 - IoU: 0.9663 - val_loss: 0.0312 - val_accuracy: 1.0000 - val_recall: 0.9470 - val_precision: 0.9900 - val_f1_score: 0.9696 - val_IoU: 0.9543\n",
      "Epoch 97/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9698 - f1_score: 0.9702 - IoU: 0.9651\n",
      "Epoch 97: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 32s 754ms/step - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9698 - f1_score: 0.9702 - IoU: 0.9651 - val_loss: 0.0250 - val_accuracy: 1.0000 - val_recall: 0.9641 - val_precision: 0.9888 - val_f1_score: 0.9756 - val_IoU: 0.9630\n",
      "Epoch 98/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9745 - precision: 0.9677 - f1_score: 0.9705 - IoU: 0.9662\n",
      "Epoch 98: val_f1_score did not improve from 0.98015\n",
      "42/42 [==============================] - 51s 1s/step - loss: 0.0298 - accuracy: 1.0000 - recall: 0.9745 - precision: 0.9677 - f1_score: 0.9705 - IoU: 0.9662 - val_loss: 0.0254 - val_accuracy: 1.0000 - val_recall: 0.9629 - val_precision: 0.9920 - val_f1_score: 0.9762 - val_IoU: 0.9631\n",
      "Epoch 99/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000 - recall: 0.9727 - precision: 0.9712 - f1_score: 0.9703 - IoU: 0.9653\n",
      "Epoch 99: val_f1_score improved from 0.98015 to 0.98117, saving model to Trans UNet DICE 5-fold model/model_5fold.keras\n",
      "42/42 [==============================] - 37s 893ms/step - loss: 0.0301 - accuracy: 1.0000 - recall: 0.9727 - precision: 0.9712 - f1_score: 0.9703 - IoU: 0.9653 - val_loss: 0.0194 - val_accuracy: 1.0000 - val_recall: 0.9808 - val_precision: 0.9850 - val_f1_score: 0.9812 - val_IoU: 0.9758\n",
      "Epoch 100/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9704 - f1_score: 0.9725 - IoU: 0.9671\n",
      "Epoch 100: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 39s 927ms/step - loss: 0.0279 - accuracy: 1.0000 - recall: 0.9748 - precision: 0.9704 - f1_score: 0.9725 - IoU: 0.9671 - val_loss: 0.0236 - val_accuracy: 1.0000 - val_recall: 0.9909 - val_precision: 0.9706 - val_f1_score: 0.9762 - val_IoU: 0.9817\n",
      "Epoch 101/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9701 - f1_score: 0.9716 - IoU: 0.9665\n",
      "Epoch 101: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 69s 2s/step - loss: 0.0287 - accuracy: 1.0000 - recall: 0.9751 - precision: 0.9701 - f1_score: 0.9716 - IoU: 0.9665 - val_loss: 0.0199 - val_accuracy: 1.0000 - val_recall: 0.9850 - val_precision: 0.9808 - val_f1_score: 0.9803 - val_IoU: 0.9771\n",
      "Epoch 102/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9734 - f1_score: 0.9718 - IoU: 0.9653\n",
      "Epoch 102: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 56s 1s/step - loss: 0.0285 - accuracy: 1.0000 - recall: 0.9721 - precision: 0.9734 - f1_score: 0.9718 - IoU: 0.9653 - val_loss: 0.0213 - val_accuracy: 1.0000 - val_recall: 0.9762 - val_precision: 0.9872 - val_f1_score: 0.9790 - val_IoU: 0.9706\n",
      "Epoch 103/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9760 - precision: 0.9697 - f1_score: 0.9699 - IoU: 0.9675\n",
      "Epoch 103: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 710ms/step - loss: 0.0305 - accuracy: 1.0000 - recall: 0.9760 - precision: 0.9697 - f1_score: 0.9699 - IoU: 0.9675 - val_loss: 0.0189 - val_accuracy: 1.0000 - val_recall: 0.9856 - val_precision: 0.9804 - val_f1_score: 0.9809 - val_IoU: 0.9789\n",
      "Epoch 104/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000 - recall: 0.9752 - precision: 0.9715 - f1_score: 0.9732 - IoU: 0.9674\n",
      "Epoch 104: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 688ms/step - loss: 0.0272 - accuracy: 1.0000 - recall: 0.9752 - precision: 0.9715 - f1_score: 0.9732 - IoU: 0.9674 - val_loss: 0.0215 - val_accuracy: 1.0000 - val_recall: 0.9833 - val_precision: 0.9808 - val_f1_score: 0.9785 - val_IoU: 0.9739\n",
      "Epoch 105/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9694 - f1_score: 0.9729 - IoU: 0.9657\n",
      "Epoch 105: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 695ms/step - loss: 0.0274 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9694 - f1_score: 0.9729 - IoU: 0.9657 - val_loss: 0.0249 - val_accuracy: 1.0000 - val_recall: 0.9591 - val_precision: 0.9923 - val_f1_score: 0.9754 - val_IoU: 0.9608\n",
      "Epoch 106/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9725 - f1_score: 0.9731 - IoU: 0.9673\n",
      "Epoch 106: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 690ms/step - loss: 0.0272 - accuracy: 1.0000 - recall: 0.9755 - precision: 0.9725 - f1_score: 0.9731 - IoU: 0.9673 - val_loss: 0.0215 - val_accuracy: 1.0000 - val_recall: 0.9716 - val_precision: 0.9902 - val_f1_score: 0.9796 - val_IoU: 0.9695\n",
      "Epoch 107/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 1.0000 - recall: 0.9778 - precision: 0.9704 - f1_score: 0.9742 - IoU: 0.9681\n",
      "Epoch 107: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 709ms/step - loss: 0.0261 - accuracy: 1.0000 - recall: 0.9778 - precision: 0.9704 - f1_score: 0.9742 - IoU: 0.9681 - val_loss: 0.0270 - val_accuracy: 1.0000 - val_recall: 0.9960 - val_precision: 0.9575 - val_f1_score: 0.9726 - val_IoU: 0.9841\n",
      "Epoch 108/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9771 - precision: 0.9698 - f1_score: 0.9726 - IoU: 0.9678\n",
      "Epoch 108: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 709ms/step - loss: 0.0277 - accuracy: 1.0000 - recall: 0.9771 - precision: 0.9698 - f1_score: 0.9726 - IoU: 0.9678 - val_loss: 0.0210 - val_accuracy: 1.0000 - val_recall: 0.9843 - val_precision: 0.9818 - val_f1_score: 0.9803 - val_IoU: 0.9767\n",
      "Epoch 109/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9719 - f1_score: 0.9707 - IoU: 0.9649\n",
      "Epoch 109: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 705ms/step - loss: 0.0296 - accuracy: 1.0000 - recall: 0.9717 - precision: 0.9719 - f1_score: 0.9707 - IoU: 0.9649 - val_loss: 0.0217 - val_accuracy: 1.0000 - val_recall: 0.9921 - val_precision: 0.9721 - val_f1_score: 0.9786 - val_IoU: 0.9819\n",
      "Epoch 110/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 1.0000 - recall: 0.9764 - precision: 0.9707 - f1_score: 0.9727 - IoU: 0.9678\n",
      "Epoch 110: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 702ms/step - loss: 0.0276 - accuracy: 1.0000 - recall: 0.9764 - precision: 0.9707 - f1_score: 0.9727 - IoU: 0.9678 - val_loss: 0.0207 - val_accuracy: 1.0000 - val_recall: 0.9719 - val_precision: 0.9900 - val_f1_score: 0.9790 - val_IoU: 0.9688\n",
      "Epoch 111/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9723 - precision: 0.9708 - f1_score: 0.9713 - IoU: 0.9648\n",
      "Epoch 111: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 28s 665ms/step - loss: 0.0289 - accuracy: 1.0000 - recall: 0.9723 - precision: 0.9708 - f1_score: 0.9713 - IoU: 0.9648 - val_loss: 0.0360 - val_accuracy: 1.0000 - val_recall: 0.9379 - val_precision: 0.9953 - val_f1_score: 0.9649 - val_IoU: 0.9483\n",
      "Epoch 112/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9664 - precision: 0.9614 - f1_score: 0.9643 - IoU: 0.9594\n",
      "Epoch 112: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 707ms/step - loss: 0.0359 - accuracy: 1.0000 - recall: 0.9664 - precision: 0.9614 - f1_score: 0.9643 - IoU: 0.9594 - val_loss: 0.2014 - val_accuracy: 0.9998 - val_recall: 0.8694 - val_precision: 0.8409 - val_f1_score: 0.7964 - val_IoU: 0.8776\n",
      "Epoch 113/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9999 - recall: 0.9118 - precision: 0.9198 - f1_score: 0.9137 - IoU: 0.9170\n",
      "Epoch 113: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 691ms/step - loss: 0.0868 - accuracy: 0.9999 - recall: 0.9118 - precision: 0.9198 - f1_score: 0.9137 - IoU: 0.9170 - val_loss: 0.1353 - val_accuracy: 0.9998 - val_recall: 0.9277 - val_precision: 0.8335 - val_f1_score: 0.8704 - val_IoU: 0.9096\n",
      "Epoch 114/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9999 - recall: 0.9471 - precision: 0.9519 - f1_score: 0.9481 - IoU: 0.9446\n",
      "Epoch 114: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 688ms/step - loss: 0.0524 - accuracy: 0.9999 - recall: 0.9471 - precision: 0.9519 - f1_score: 0.9481 - IoU: 0.9446 - val_loss: 0.1042 - val_accuracy: 0.9999 - val_recall: 0.8735 - val_precision: 0.9246 - val_f1_score: 0.9009 - val_IoU: 0.9051\n",
      "Epoch 115/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9999 - recall: 0.9518 - precision: 0.9586 - f1_score: 0.9537 - IoU: 0.9500\n",
      "Epoch 115: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 690ms/step - loss: 0.0467 - accuracy: 0.9999 - recall: 0.9518 - precision: 0.9586 - f1_score: 0.9537 - IoU: 0.9500 - val_loss: 0.0375 - val_accuracy: 1.0000 - val_recall: 0.9902 - val_precision: 0.9403 - val_f1_score: 0.9633 - val_IoU: 0.9748\n",
      "Epoch 116/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 1.0000 - recall: 0.9612 - precision: 0.9660 - f1_score: 0.9624 - IoU: 0.9569\n",
      "Epoch 116: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 732ms/step - loss: 0.0380 - accuracy: 1.0000 - recall: 0.9612 - precision: 0.9660 - f1_score: 0.9624 - IoU: 0.9569 - val_loss: 0.0277 - val_accuracy: 1.0000 - val_recall: 0.9868 - val_precision: 0.9632 - val_f1_score: 0.9729 - val_IoU: 0.9778\n",
      "Epoch 117/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0346 - accuracy: 1.0000 - recall: 0.9682 - precision: 0.9634 - f1_score: 0.9657 - IoU: 0.9607\n",
      "Epoch 117: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 33s 781ms/step - loss: 0.0346 - accuracy: 1.0000 - recall: 0.9682 - precision: 0.9634 - f1_score: 0.9657 - IoU: 0.9607 - val_loss: 0.0284 - val_accuracy: 1.0000 - val_recall: 0.9647 - val_precision: 0.9845 - val_f1_score: 0.9725 - val_IoU: 0.9635\n",
      "Epoch 118/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 1.0000 - recall: 0.9707 - precision: 0.9655 - f1_score: 0.9665 - IoU: 0.9621\n",
      "Epoch 118: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 703ms/step - loss: 0.0337 - accuracy: 1.0000 - recall: 0.9707 - precision: 0.9655 - f1_score: 0.9665 - IoU: 0.9621 - val_loss: 0.0254 - val_accuracy: 1.0000 - val_recall: 0.9894 - val_precision: 0.9653 - val_f1_score: 0.9756 - val_IoU: 0.9797\n",
      "Epoch 119/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9680 - f1_score: 0.9644 - IoU: 0.9600\n",
      "Epoch 119: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 689ms/step - loss: 0.0358 - accuracy: 1.0000 - recall: 0.9657 - precision: 0.9680 - f1_score: 0.9644 - IoU: 0.9600 - val_loss: 0.0300 - val_accuracy: 1.0000 - val_recall: 0.9481 - val_precision: 0.9933 - val_f1_score: 0.9706 - val_IoU: 0.9540\n",
      "Epoch 120/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9641 - f1_score: 0.9654 - IoU: 0.9611\n",
      "Epoch 120: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 724ms/step - loss: 0.0350 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9641 - f1_score: 0.9654 - IoU: 0.9611 - val_loss: 0.1213 - val_accuracy: 0.9999 - val_recall: 0.7574 - val_precision: 0.9938 - val_f1_score: 0.8843 - val_IoU: 0.8544\n",
      "Epoch 121/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9999 - recall: 0.9465 - precision: 0.9459 - f1_score: 0.9416 - IoU: 0.9420\n",
      "Epoch 121: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 30s 722ms/step - loss: 0.0588 - accuracy: 0.9999 - recall: 0.9465 - precision: 0.9459 - f1_score: 0.9416 - IoU: 0.9420 - val_loss: 0.5542 - val_accuracy: 0.9995 - val_recall: 0.2454 - val_precision: 0.9974 - val_f1_score: 0.4700 - val_IoU: 0.6015\n",
      "Epoch 122/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9623 - f1_score: 0.9649 - IoU: 0.9616\n",
      "Epoch 122: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 740ms/step - loss: 0.0354 - accuracy: 1.0000 - recall: 0.9690 - precision: 0.9623 - f1_score: 0.9649 - IoU: 0.9616 - val_loss: 0.2584 - val_accuracy: 0.9997 - val_recall: 0.5593 - val_precision: 0.9930 - val_f1_score: 0.7539 - val_IoU: 0.7445\n",
      "Epoch 123/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 1.0000 - recall: 0.9696 - precision: 0.9657 - f1_score: 0.9671 - IoU: 0.9625\n",
      "Epoch 123: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 704ms/step - loss: 0.0332 - accuracy: 1.0000 - recall: 0.9696 - precision: 0.9657 - f1_score: 0.9671 - IoU: 0.9625 - val_loss: 0.0408 - val_accuracy: 1.0000 - val_recall: 0.9543 - val_precision: 0.9800 - val_f1_score: 0.9613 - val_IoU: 0.9585\n",
      "Epoch 124/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9671 - f1_score: 0.9676 - IoU: 0.9621\n",
      "Epoch 124: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 684ms/step - loss: 0.0328 - accuracy: 1.0000 - recall: 0.9694 - precision: 0.9671 - f1_score: 0.9676 - IoU: 0.9621 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_recall: 0.9698 - val_precision: 0.9828 - val_f1_score: 0.9737 - val_IoU: 0.9661\n",
      "Epoch 125/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9689 - precision: 0.9660 - f1_score: 0.9674 - IoU: 0.9628\n",
      "Epoch 125: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 702ms/step - loss: 0.0327 - accuracy: 1.0000 - recall: 0.9689 - precision: 0.9660 - f1_score: 0.9674 - IoU: 0.9628 - val_loss: 0.0258 - val_accuracy: 1.0000 - val_recall: 0.9687 - val_precision: 0.9845 - val_f1_score: 0.9757 - val_IoU: 0.9670\n",
      "Epoch 126/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000 - recall: 0.9700 - precision: 0.9671 - f1_score: 0.9679 - IoU: 0.9637\n",
      "Epoch 126: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 691ms/step - loss: 0.0323 - accuracy: 1.0000 - recall: 0.9700 - precision: 0.9671 - f1_score: 0.9679 - IoU: 0.9637 - val_loss: 0.0305 - val_accuracy: 1.0000 - val_recall: 0.9820 - val_precision: 0.9718 - val_f1_score: 0.9693 - val_IoU: 0.9722\n",
      "Epoch 127/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9722 - precision: 0.9673 - f1_score: 0.9700 - IoU: 0.9647\n",
      "Epoch 127: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 33s 794ms/step - loss: 0.0303 - accuracy: 1.0000 - recall: 0.9722 - precision: 0.9673 - f1_score: 0.9700 - IoU: 0.9647 - val_loss: 0.0978 - val_accuracy: 0.9999 - val_recall: 0.8018 - val_precision: 0.9933 - val_f1_score: 0.9055 - val_IoU: 0.8758\n",
      "Epoch 128/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9680 - f1_score: 0.9702 - IoU: 0.9650\n",
      "Epoch 128: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 700ms/step - loss: 0.0300 - accuracy: 1.0000 - recall: 0.9729 - precision: 0.9680 - f1_score: 0.9702 - IoU: 0.9650 - val_loss: 0.0285 - val_accuracy: 1.0000 - val_recall: 0.9523 - val_precision: 0.9924 - val_f1_score: 0.9713 - val_IoU: 0.9588\n",
      "Epoch 129/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9706 - precision: 0.9689 - f1_score: 0.9692 - IoU: 0.9644\n",
      "Epoch 129: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 693ms/step - loss: 0.0310 - accuracy: 1.0000 - recall: 0.9706 - precision: 0.9689 - f1_score: 0.9692 - IoU: 0.9644 - val_loss: 0.0291 - val_accuracy: 1.0000 - val_recall: 0.9565 - val_precision: 0.9899 - val_f1_score: 0.9716 - val_IoU: 0.9585\n",
      "Epoch 130/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 1.0000 - recall: 0.9736 - precision: 0.9637 - f1_score: 0.9676 - IoU: 0.9655\n",
      "Epoch 130: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 48s 1s/step - loss: 0.0326 - accuracy: 1.0000 - recall: 0.9736 - precision: 0.9637 - f1_score: 0.9676 - IoU: 0.9655 - val_loss: 0.0216 - val_accuracy: 1.0000 - val_recall: 0.9863 - val_precision: 0.9754 - val_f1_score: 0.9783 - val_IoU: 0.9784\n",
      "Epoch 131/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 1.0000 - recall: 0.9711 - precision: 0.9704 - f1_score: 0.9707 - IoU: 0.9650\n",
      "Epoch 131: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 33s 792ms/step - loss: 0.0294 - accuracy: 1.0000 - recall: 0.9711 - precision: 0.9704 - f1_score: 0.9707 - IoU: 0.9650 - val_loss: 0.0219 - val_accuracy: 1.0000 - val_recall: 0.9804 - val_precision: 0.9808 - val_f1_score: 0.9780 - val_IoU: 0.9751\n",
      "Epoch 132/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9720 - precision: 0.9696 - f1_score: 0.9706 - IoU: 0.9654\n",
      "Epoch 132: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 55s 1s/step - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9720 - precision: 0.9696 - f1_score: 0.9706 - IoU: 0.9654 - val_loss: 0.0248 - val_accuracy: 1.0000 - val_recall: 0.9640 - val_precision: 0.9908 - val_f1_score: 0.9749 - val_IoU: 0.9654\n",
      "Epoch 133/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9687 - f1_score: 0.9719 - IoU: 0.9674\n",
      "Epoch 133: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 41s 991ms/step - loss: 0.0283 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9687 - f1_score: 0.9719 - IoU: 0.9674 - val_loss: 0.0199 - val_accuracy: 1.0000 - val_recall: 0.9824 - val_precision: 0.9825 - val_f1_score: 0.9803 - val_IoU: 0.9771\n",
      "Epoch 134/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9707 - precision: 0.9718 - f1_score: 0.9705 - IoU: 0.9647\n",
      "Epoch 134: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 706ms/step - loss: 0.0295 - accuracy: 1.0000 - recall: 0.9707 - precision: 0.9718 - f1_score: 0.9705 - IoU: 0.9647 - val_loss: 0.0233 - val_accuracy: 1.0000 - val_recall: 0.9746 - val_precision: 0.9839 - val_f1_score: 0.9765 - val_IoU: 0.9714\n",
      "Epoch 135/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9703 - f1_score: 0.9718 - IoU: 0.9663\n",
      "Epoch 135: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 58s 1s/step - loss: 0.0285 - accuracy: 1.0000 - recall: 0.9737 - precision: 0.9703 - f1_score: 0.9718 - IoU: 0.9663 - val_loss: 0.0229 - val_accuracy: 1.0000 - val_recall: 0.9711 - val_precision: 0.9864 - val_f1_score: 0.9774 - val_IoU: 0.9683\n",
      "Epoch 136/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000 - recall: 0.9699 - precision: 0.9709 - f1_score: 0.9683 - IoU: 0.9642\n",
      "Epoch 136: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 34s 777ms/step - loss: 0.0319 - accuracy: 1.0000 - recall: 0.9699 - precision: 0.9709 - f1_score: 0.9683 - IoU: 0.9642 - val_loss: 0.0266 - val_accuracy: 1.0000 - val_recall: 0.9887 - val_precision: 0.9661 - val_f1_score: 0.9739 - val_IoU: 0.9781\n",
      "Epoch 137/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000 - recall: 0.9743 - precision: 0.9696 - f1_score: 0.9705 - IoU: 0.9663\n",
      "Epoch 137: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 34s 804ms/step - loss: 0.0297 - accuracy: 1.0000 - recall: 0.9743 - precision: 0.9696 - f1_score: 0.9705 - IoU: 0.9663 - val_loss: 0.0259 - val_accuracy: 1.0000 - val_recall: 0.9921 - val_precision: 0.9606 - val_f1_score: 0.9743 - val_IoU: 0.9798\n",
      "Epoch 138/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000 - recall: 0.9715 - precision: 0.9711 - f1_score: 0.9716 - IoU: 0.9650\n",
      "Epoch 138: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 748ms/step - loss: 0.0287 - accuracy: 1.0000 - recall: 0.9715 - precision: 0.9711 - f1_score: 0.9716 - IoU: 0.9650 - val_loss: 0.0289 - val_accuracy: 1.0000 - val_recall: 0.9530 - val_precision: 0.9920 - val_f1_score: 0.9711 - val_IoU: 0.9569\n",
      "Epoch 139/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9693 - precision: 0.9699 - f1_score: 0.9698 - IoU: 0.9648\n",
      "Epoch 139: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 738ms/step - loss: 0.0304 - accuracy: 1.0000 - recall: 0.9693 - precision: 0.9699 - f1_score: 0.9698 - IoU: 0.9648 - val_loss: 0.0271 - val_accuracy: 1.0000 - val_recall: 0.9938 - val_precision: 0.9558 - val_f1_score: 0.9726 - val_IoU: 0.9808\n",
      "Epoch 140/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9699 - f1_score: 0.9691 - IoU: 0.9668\n",
      "Epoch 140: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 32s 756ms/step - loss: 0.0311 - accuracy: 1.0000 - recall: 0.9742 - precision: 0.9699 - f1_score: 0.9691 - IoU: 0.9668 - val_loss: 0.0211 - val_accuracy: 1.0000 - val_recall: 0.9777 - val_precision: 0.9848 - val_f1_score: 0.9786 - val_IoU: 0.9734\n",
      "Epoch 141/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9725 - f1_score: 0.9745 - IoU: 0.9695\n",
      "Epoch 141: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 744ms/step - loss: 0.0256 - accuracy: 1.0000 - recall: 0.9761 - precision: 0.9725 - f1_score: 0.9745 - IoU: 0.9695 - val_loss: 0.0204 - val_accuracy: 1.0000 - val_recall: 0.9859 - val_precision: 0.9774 - val_f1_score: 0.9797 - val_IoU: 0.9794\n",
      "Epoch 142/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 1.0000 - recall: 0.9776 - precision: 0.9716 - f1_score: 0.9747 - IoU: 0.9696\n",
      "Epoch 142: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 33s 783ms/step - loss: 0.0256 - accuracy: 1.0000 - recall: 0.9776 - precision: 0.9716 - f1_score: 0.9747 - IoU: 0.9696 - val_loss: 0.0203 - val_accuracy: 1.0000 - val_recall: 0.9897 - val_precision: 0.9749 - val_f1_score: 0.9795 - val_IoU: 0.9817\n",
      "Epoch 143/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9721 - f1_score: 0.9733 - IoU: 0.9697\n",
      "Epoch 143: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 33s 783ms/step - loss: 0.0269 - accuracy: 1.0000 - recall: 0.9765 - precision: 0.9721 - f1_score: 0.9733 - IoU: 0.9697 - val_loss: 0.0201 - val_accuracy: 1.0000 - val_recall: 0.9786 - val_precision: 0.9860 - val_f1_score: 0.9799 - val_IoU: 0.9758\n",
      "Epoch 144/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9757 - precision: 0.9725 - f1_score: 0.9739 - IoU: 0.9687\n",
      "Epoch 144: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 32s 759ms/step - loss: 0.0263 - accuracy: 1.0000 - recall: 0.9757 - precision: 0.9725 - f1_score: 0.9739 - IoU: 0.9687 - val_loss: 0.0205 - val_accuracy: 1.0000 - val_recall: 0.9930 - val_precision: 0.9699 - val_f1_score: 0.9795 - val_IoU: 0.9847\n",
      "Epoch 145/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9999 - recall: 0.9188 - precision: 0.9330 - f1_score: 0.9242 - IoU: 0.9248\n",
      "Epoch 145: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 32s 769ms/step - loss: 0.0765 - accuracy: 0.9999 - recall: 0.9188 - precision: 0.9330 - f1_score: 0.9242 - IoU: 0.9248 - val_loss: 0.4668 - val_accuracy: 0.9996 - val_recall: 0.3688 - val_precision: 0.9737 - val_f1_score: 0.5496 - val_IoU: 0.6685\n",
      "Epoch 146/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9999 - recall: 0.9609 - precision: 0.9556 - f1_score: 0.9577 - IoU: 0.9541\n",
      "Epoch 146: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 32s 752ms/step - loss: 0.0427 - accuracy: 0.9999 - recall: 0.9609 - precision: 0.9556 - f1_score: 0.9577 - IoU: 0.9541 - val_loss: 0.1057 - val_accuracy: 0.9999 - val_recall: 0.8425 - val_precision: 0.9697 - val_f1_score: 0.8985 - val_IoU: 0.9053\n",
      "Epoch 147/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000 - recall: 0.9673 - precision: 0.9627 - f1_score: 0.9645 - IoU: 0.9605\n",
      "Epoch 147: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 694ms/step - loss: 0.0356 - accuracy: 1.0000 - recall: 0.9673 - precision: 0.9627 - f1_score: 0.9645 - IoU: 0.9605 - val_loss: 0.1022 - val_accuracy: 0.9999 - val_recall: 0.8338 - val_precision: 0.9834 - val_f1_score: 0.9021 - val_IoU: 0.9016\n",
      "Epoch 148/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9999 - recall: 0.9267 - precision: 0.9217 - f1_score: 0.9209 - IoU: 0.9269\n",
      "Epoch 148: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 28s 681ms/step - loss: 0.0794 - accuracy: 0.9999 - recall: 0.9267 - precision: 0.9217 - f1_score: 0.9209 - IoU: 0.9269 - val_loss: 0.3934 - val_accuracy: 0.9994 - val_recall: 0.8584 - val_precision: 0.4944 - val_f1_score: 0.6171 - val_IoU: 0.7697\n",
      "Epoch 149/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9999 - recall: 0.9308 - precision: 0.9087 - f1_score: 0.9157 - IoU: 0.9206\n",
      "Epoch 149: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 29s 685ms/step - loss: 0.0846 - accuracy: 0.9999 - recall: 0.9308 - precision: 0.9087 - f1_score: 0.9157 - IoU: 0.9206 - val_loss: 0.1009 - val_accuracy: 0.9999 - val_recall: 0.8847 - val_precision: 0.9532 - val_f1_score: 0.9063 - val_IoU: 0.9099\n",
      "Epoch 150/150\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9999 - recall: 0.9354 - precision: 0.9423 - f1_score: 0.9346 - IoU: 0.9370\n",
      "Epoch 150: val_f1_score did not improve from 0.98117\n",
      "42/42 [==============================] - 31s 732ms/step - loss: 0.0659 - accuracy: 0.9999 - recall: 0.9354 - precision: 0.9423 - f1_score: 0.9346 - IoU: 0.9370 - val_loss: 0.1008 - val_accuracy: 0.9999 - val_recall: 0.9910 - val_precision: 0.8812 - val_f1_score: 0.9034 - val_IoU: 0.9524\n",
      "O modelo demorou 4948.99 segundos para treinar.\n",
      "Métricas salvas com sucesso na pasta: Trans UNet DICE 5-fold model\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "jacc = []\n",
    "f1 = []\n",
    "prec = []\n",
    "rec = []\n",
    "\n",
    "# Dicionário para armazenar métricas por fold\n",
    "fold_metrics_summary = []\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "fold = 4\n",
    "\n",
    "for i, (train_ind, test_ind) in enumerate(kf.split(X)):\n",
    "    if i < (fold-1):\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    print(\"Fold: \" + str(fold))\n",
    "    X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "\n",
    "    model = trans_unet()\n",
    "    \n",
    "    \n",
    "    checkpoint_filepath = os.path.join(output_dir, f'model_{fold}fold.keras')\n",
    "    callbacks = [\n",
    "            tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_filepath,\n",
    "                save_weights_only=False,\n",
    "                monitor='val_f1_score',\n",
    "                mode='max',\n",
    "                save_best_only=True,\n",
    "                verbose=1)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data=(X_val,y_val), batch_size=16, epochs=150, callbacks=callbacks)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "     # Salvando o tempo de treinamento\n",
    "    with open(os.path.join(output_dir, 'training_time.txt'), 'a') as f:\n",
    "        f.write(f'Fold {fold}: {training_time:.2f} segundos\\n')\n",
    "    print(f\"O modelo demorou {training_time:.2f} segundos para treinar.\")\n",
    "    \n",
    "    loss = model.history.history['loss']\n",
    "    val_loss = model.history.history['val_loss']\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'loss_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Epoch\", \"Loss\", \"Validation Loss\"])\n",
    "        for epoch, (l, vl) in enumerate(zip(loss, val_loss), start=1):\n",
    "            writer.writerow([epoch, l, vl])\n",
    "            \n",
    "     # Plotando e salvando a figura\n",
    "    plt.figure()\n",
    "    plt.plot(loss, 'r', label='Training loss')\n",
    "    plt.plot(val_loss, 'g', label='Validation loss')\n",
    "    plt.title(f'Training and Validation Loss - Fold {fold}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_plot_fold{fold}.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    acc_fold = []\n",
    "    jacc_fold = []\n",
    "    f1_fold = []\n",
    "    prec_fold = []\n",
    "    rec_fold = []\n",
    "    \n",
    "    for i in range(0, len(X_val)):\n",
    "        sample_image = X_val[i]\n",
    "        sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "        prediction = model.predict(sample_image[tf.newaxis, ...],verbose=0)[0]\n",
    "        predicted_mask = (prediction > 0.5).astype(np.uint8).flatten()\n",
    "            \n",
    "        acc_score = accuracy_score(sample_mask, predicted_mask)\n",
    "        jacc_score = jaccard_score(sample_mask, predicted_mask)\n",
    "        f1_score_val = f1_score(sample_mask, predicted_mask).numpy()\n",
    "        prec_score = precision_score(sample_mask, predicted_mask)\n",
    "        rec_score = recall_score(sample_mask, predicted_mask)\n",
    "\n",
    "        acc_fold.append(acc_score)\n",
    "        jacc_fold.append(jacc_score)\n",
    "        f1_fold.append(f1_score_val)\n",
    "        prec_fold.append(prec_score)\n",
    "        rec_fold.append(rec_score)\n",
    "\n",
    "        fold_metrics.append([i, acc_score, jacc_score, f1_score_val, prec_score, rec_score])\n",
    "        \n",
    "    # Salvando métricas por imagem\n",
    "    with open(os.path.join(output_dir, f'metrics_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Index\", \"Accuracy\", \"Jaccard\", \"DICE\", \"Precision\", \"Recall\"])\n",
    "        writer.writerows(fold_metrics)\n",
    "\n",
    "    # Salvando a média do fold para cada métrica\n",
    "    fold_metrics_summary.append([\n",
    "        fold, np.mean(acc_fold), np.std(acc_fold),\n",
    "        np.mean(jacc_fold), np.std(jacc_fold),\n",
    "        np.mean(f1_fold), np.std(f1_fold),\n",
    "        np.mean(prec_fold), np.std(prec_fold),\n",
    "        np.mean(rec_fold), np.std(rec_fold)\n",
    "    ])\n",
    "\n",
    "        \n",
    "    del model \n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    fold+=1\n",
    "    \n",
    "# Calculando médias gerais\n",
    "acc_total = [m[1] for m in fold_metrics_summary]\n",
    "jacc_total = [m[3] for m in fold_metrics_summary]\n",
    "f1_total = [m[5] for m in fold_metrics_summary]\n",
    "prec_total = [m[7] for m in fold_metrics_summary]\n",
    "rec_total = [m[9] for m in fold_metrics_summary]\n",
    "\n",
    "general_summary = [\n",
    "    [\"Fold\", \"Accuracy Mean\", \"Accuracy Std\", \"Jaccard Mean\", \"Jaccard Std\",\n",
    "     \"DICE Mean\", \"DICE Std\", \"Precision Mean\", \"Precision Std\", \"Recall Mean\", \"Recall Std\"]\n",
    "]\n",
    "\n",
    "general_summary.extend(fold_metrics_summary)\n",
    "\n",
    "general_summary.append([\n",
    "    \"Overall Mean\", np.mean(acc_total), np.std(acc_total),\n",
    "    np.mean(jacc_total), np.std(jacc_total),\n",
    "    np.mean(f1_total), np.std(f1_total),\n",
    "    np.mean(prec_total), np.std(prec_total),\n",
    "    np.mean(rec_total), np.std(rec_total)\n",
    "])\n",
    "\n",
    "# Salvando o resumo das métricas por fold e geral\n",
    "with open(os.path.join(output_dir, 'metrics_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    writer.writerows(general_summary)\n",
    "\n",
    "print(\"Métricas salvas com sucesso na pasta:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5d184f-5b31-4156-849f-d24ddac7c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k-fold:  20%|███████████████▍                                                             | 1/5 [00:51<03:25, 51.34s/it]/home/mbouzon/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "k-fold:  40%|██████████████████████████████▊                                              | 2/5 [01:42<02:33, 51.00s/it]/home/mbouzon/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "k-fold:  80%|█████████████████████████████████████████████████████████████▌               | 4/5 [03:15<00:48, 48.41s/it]/home/mbouzon/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "k-fold: 100%|█████████████████████████████████████████████████████████████████████████████| 5/5 [04:07<00:00, 49.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas salvas com sucesso na pasta: Trans UNet DICE 5-fold model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc_mean_fold = []\n",
    "jacc_mean_fold = []\n",
    "f1_mean_fold = []\n",
    "prec_mean_fold = []\n",
    "rec_mean_fold = []\n",
    "\n",
    "acc_std_fold = []\n",
    "jacc_std_fold = []\n",
    "f1_std_fold = []\n",
    "prec_std_fold = []\n",
    "rec_std_fold = []\n",
    "\n",
    "\n",
    "# Dicionário para armazenar métricas por fold\n",
    "fold_metrics_summary = []\n",
    "\n",
    "fold = 1\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "for train_ind, test_ind in tqdm(kf.split(X), total=kf.get_n_splits(), desc=\"k-fold\"):\n",
    "    X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "\n",
    "    fold_metrics = []\n",
    "\n",
    "    acc_fold = []\n",
    "    jacc_fold = []\n",
    "    f1_fold = []\n",
    "    prec_fold = []\n",
    "    rec_fold = []\n",
    "    \n",
    "    model_filepath = filepath = os.path.join(output_dir, f'model_{fold}fold.keras')\n",
    "    model = tf.keras.models.load_model(model_filepath)\n",
    "\n",
    "    for i in range(0, len(X_val)):\n",
    "        sample_image = X_val[i]\n",
    "        sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "        prediction = model.predict(sample_image[tf.newaxis, ...],verbose=0)[0]\n",
    "        predicted_mask = (prediction > 0.5).astype(np.uint8).flatten()\n",
    "            \n",
    "        acc_score = accuracy_score(sample_mask, predicted_mask)\n",
    "        jacc_score = jaccard_score(sample_mask, predicted_mask)\n",
    "        f1_score_val = f1_score(sample_mask, predicted_mask).numpy()\n",
    "        prec_score = precision_score(sample_mask, predicted_mask)\n",
    "        rec_score = recall_score(sample_mask, predicted_mask)\n",
    "\n",
    "        acc_fold.append(acc_score)\n",
    "        jacc_fold.append(jacc_score)\n",
    "        f1_fold.append(f1_score_val)\n",
    "        prec_fold.append(prec_score)\n",
    "        rec_fold.append(rec_score)\n",
    "\n",
    "        fold_metrics.append([i, acc_score, jacc_score, f1_score_val, prec_score, rec_score])\n",
    "        \n",
    "    # Salvando métricas por imagem\n",
    "    with open(os.path.join(output_dir, f'metrics_fold{fold}.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=';')\n",
    "        writer.writerow([\"Index\", \"Accuracy\", \"Jaccard\", \"DICE\", \"Precision\", \"Recall\"])\n",
    "        writer.writerows(fold_metrics)\n",
    "\n",
    "    # Salvando a média do fold para cada métrica\n",
    "    fold_metrics_summary.append([\n",
    "        fold, np.mean(acc_fold), np.std(acc_fold),\n",
    "        np.mean(jacc_fold), np.std(jacc_fold),\n",
    "        np.mean(f1_fold), np.std(f1_fold),\n",
    "        np.mean(prec_fold), np.std(prec_fold),\n",
    "        np.mean(rec_fold), np.std(rec_fold)\n",
    "    ])\n",
    "\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# Calculando médias gerais\n",
    "acc_total = [m[1] for m in fold_metrics_summary]\n",
    "jacc_total = [m[3] for m in fold_metrics_summary]\n",
    "f1_total = [m[5] for m in fold_metrics_summary]\n",
    "prec_total = [m[7] for m in fold_metrics_summary]\n",
    "rec_total = [m[9] for m in fold_metrics_summary]\n",
    "\n",
    "general_summary = [\n",
    "    [\"Fold\", \"Accuracy Mean\", \"Accuracy Std\", \"Jaccard Mean\", \"Jaccard Std\",\n",
    "     \"DICE Mean\", \"DICE Std\", \"Precision Mean\", \"Precision Std\", \"Recall Mean\", \"Recall Std\"]\n",
    "]\n",
    "\n",
    "general_summary.extend(fold_metrics_summary)\n",
    "\n",
    "general_summary.append([\n",
    "    \"Overall Mean\", np.mean(acc_total), np.std(acc_total),\n",
    "    np.mean(jacc_total), np.std(jacc_total),\n",
    "    np.mean(f1_total), np.std(f1_total),\n",
    "    np.mean(prec_total), np.std(prec_total),\n",
    "    np.mean(rec_total), np.std(rec_total)\n",
    "])\n",
    "\n",
    "# Salvando o resumo das métricas por fold e geral\n",
    "with open(os.path.join(output_dir, 'metrics_summary.csv'), 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';')\n",
    "    writer.writerows(general_summary)\n",
    "\n",
    "print(\"Métricas salvas com sucesso na pasta:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2544ddf-696d-4ff5-8780-9a498daa8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.9974017963132 +- 0.0009768504968907241\n",
      "Jaccard: 94.18922478512137 +- 0.5817725841325734\n",
      "Dice: 96.59768342971802 +- 0.41947118006646633\n",
      "Precision: 97.022278679358 +- 0.5131467677738468\n",
      "Recall: 96.68672352455712 +- 0.7697583006408213\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \"+ str(np.mean(acc_total)*100) + \" +- \" + str(np.std(acc_total)*100))\n",
    "print(\"Jaccard: \"+ str(np.mean(jacc_total)*100) + \" +- \" + str(np.std(jacc_total)*100))\n",
    "print(\"Dice: \"+ str(np.mean(f1_total)*100) + \" +- \" + str(np.std(f1_total)*100))\n",
    "print(\"Precision: \"+ str(np.mean(prec_total)*100) + \" +- \" + str(np.std(prec_total)*100))\n",
    "print(\"Recall: \"+ str(np.mean(rec_total)*100) + \" +- \" + str(np.std(rec_total)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e774436e-ff52-47f8-8dce-d80b378b614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold = 2\n",
    "best_model_filepath = filepath = os.path.join(output_dir, f'model_{best_fold}fold.keras')\n",
    "best_model = tf.keras.models.load_model(best_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac69b19-a5f7-40f1-b395-d65cd72999d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k, shuffle=True, random_state=28)\n",
    "for i, (train_ind, test_ind) in enumerate(kf.split(X)):\n",
    "    if i == (best_fold-1):\n",
    "        X_train, X_val, y_train, y_val = X[train_ind], X[test_ind], y[train_ind], y[test_ind]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6713b66-5fa4-436e-a338-dee9851b3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input image', 'True mask', 'Predicted mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329d56e2-63cb-4232-b7eb-0a9ed7c6b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999427795410156\n",
      "Jaccard: 0.9460431654676259\n",
      "Dice: 0.9722735674676526\n",
      "Precision: 0.9850187265917603\n",
      "Recall: 0.9598540145985401\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0, len(X_val))\n",
    "sample_image = X_val[i]\n",
    "sample_mask = y_val[i]\n",
    "prediction = best_model.predict(sample_image[tf.newaxis, ...])[0]\n",
    "predicted_mask = (prediction > 0.5).astype(np.uint8)\n",
    "display([sample_image, sample_mask,predicted_mask])\n",
    "\n",
    "sample_mask = y_val[i].astype(np.uint8).flatten()\n",
    "predicted_mask = predicted_mask.flatten()\n",
    "\n",
    "\n",
    "print(\"Accuracy: \"+ str(accuracy_score(sample_mask, predicted_mask)))\n",
    "print(\"Jaccard: \"+ str(jaccard_score(sample_mask, predicted_mask)))\n",
    "print(\"Dice: \"+ str((2*jaccard_score(sample_mask, predicted_mask))/(1+jaccard_score(sample_mask, predicted_mask))))\n",
    "print(\"Precision: \"+ str(precision_score(sample_mask, predicted_mask)))\n",
    "print(\"Recall: \"+ str(recall_score(sample_mask, predicted_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4bd9e15-bc6d-4743-ae4b-ce8a3202b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 14:51:29.090953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.116493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.116530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.118428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.118456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.118469: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.223806: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.223856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.223862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-03-26 14:51:29.223871: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2025-03-26 14:51:29.223875: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2025-03-26 14:51:29.223988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-26 14:51:29.224010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = trans_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6dbbfa0-859f-42e5-b035-3f44e184865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 512, 512, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 512, 512, 16)         160       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512, 512, 16)         0         ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 512, 512, 16)         2320      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 512, 512, 16)         64        ['conv2d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 512, 512, 16)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 256, 256, 16)         0         ['re_lu[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 256, 256, 32)         4640      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 256, 256, 32)         0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 256, 256, 32)         9248      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 256, 256, 32)         128       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 256, 256, 32)         0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 128, 128, 32)         0         ['re_lu_1[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 128, 128, 64)         18496     ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 128, 128, 64)         0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 128, 128, 64)         36928     ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 128, 128, 64)         256       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 128, 128, 64)         0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 64, 64, 64)           0         ['re_lu_2[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 64, 64, 128)          73856     ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 64, 64, 128)          0         ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 64, 64, 128)          147584    ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 64, 64, 128)          512       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 64, 64, 128)          0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 128)          0         ['re_lu_3[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1024, 128)            0         ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1024, 256)            33024     ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, 1024, 256)            262144    ['dense[0][0]']               \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 1024, 256)            512       ['positional_embedding[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 1024, 256)            263168    ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 1024, 256)            0         ['multi_head_attention[0][0]',\n",
      " Lambda)                                                             'positional_embedding[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add[0][0]']\n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1024, 1024)           263168    ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1024, 256)            262400    ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 1024, 256)            0         ['dense_2[0][0]',             \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_1[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 1024, 256)            263168    ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 1024, 256)            0         ['multi_head_attention_1[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1024, 1024)           263168    ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1024, 256)            262400    ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 1024, 256)            0         ['dense_4[0][0]',             \n",
      " OpLambda)                                                           'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 1024, 256)            263168    ['layer_normalization_4[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 1024, 256)            0         ['multi_head_attention_2[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_4[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1024, 1024)           263168    ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 1024, 256)            262400    ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 1024, 256)            0         ['dense_6[0][0]',             \n",
      " OpLambda)                                                           'tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_5[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 1024, 256)            263168    ['layer_normalization_6[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 1024, 256)            0         ['multi_head_attention_3[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 1024, 256)            512       ['tf.__operators__.add_6[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1024, 1024)           263168    ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 1024, 256)            262400    ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 1024, 256)            0         ['dense_8[0][0]',             \n",
      " OpLambda)                                                           'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 32, 32, 256)          0         ['tf.__operators__.add_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 64, 64, 128)          131200    ['reshape_1[0][0]']           \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 64, 64, 256)          0         ['conv2d_transpose[0][0]',    \n",
      "                                                                     're_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 64, 64, 128)          295040    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 64, 64, 128)          0         ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 64, 64, 128)          147584    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 64, 64, 128)          512       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 64, 64, 128)          0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 128, 128, 64)         32832     ['re_lu_4[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 128, 128, 128)        0         ['conv2d_transpose_1[0][0]',  \n",
      " )                                                                   're_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 128, 128, 64)         73792     ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 128, 128, 64)         0         ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 128, 128, 64)         36928     ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 128, 128, 64)         256       ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 128, 128, 64)         0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 256, 256, 32)         8224      ['re_lu_5[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 256, 256, 64)         0         ['conv2d_transpose_2[0][0]',  \n",
      " )                                                                   're_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 256, 256, 32)         18464     ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 256, 256, 32)         0         ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 256, 256, 32)         9248      ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 256, 256, 32)         128       ['conv2d_13[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 256, 256, 32)         0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 512, 512, 16)         2064      ['re_lu_6[0][0]']             \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 512, 512, 32)         0         ['conv2d_transpose_3[0][0]',  \n",
      " )                                                                   're_lu[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 512, 512, 16)         4624      ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 512, 512, 16)         0         ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 512, 512, 16)         2320      ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 512, 512, 16)         64        ['conv2d_15[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 512, 512, 16)         0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 512, 512, 1)          17        ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4511697 (17.21 MB)\n",
      "Trainable params: 4510737 (17.21 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed749349-1672-4ff4-abe8-60191b7cfe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
